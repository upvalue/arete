// compile-x64.cpp.dasc - Compile bytecode to native amd64 code
// This is a DynASM file. Must be run through DynASM to produce a C++ file

// Further reading
// Agner Fog, Calling Conventions
// http://www.agner.org/optimize/calling_conventions.pdf

// Hint: If it's crashing for no apparent reason, it's probably because the stack
// size is screwed up

// This discusses the Windows struct return issue, but doesn't
// seem to indicate why functions returning Value (only 64 bits) 
// seem to use a different calling convention
// https://msdn.microsoft.com/en-us/library/7572ztz4.aspx

// TODO: GC frames can be trivially omitted in certain cases; if a function
// does not have upvalues, never calls other functions or allocation primitives

// A function like say, memv or assq might benefit heavily from this, but we'd probably need to
// inline recursive calls in the Scheme compiler in order to make it easy to determine whether this
// is possible here.

// TODO: Why use rStackI? Why not just use the RSP and decrement/increment by 8? Would probably
// reduce code quite a bit

// TODO: Currently variables are GC-tracked much in the way we track them C++-side. But in assembly
// we have full control over the stack. Rather than using the NativeFrame structure, for example,
// we could just guarantee that everything from rsp to rsp+whatever is a pointer. One issue with the
// current approach vs VMFrames is that all stack values will be tracked while a function is live, 
// which means some garbage will remain. Imagine something like this being a memory leak:
// (let ()
//   ;; really complex computation that makes a big list
//   ;; main loop of a program that doesn't clear the stack
// )
// In practice, probably not a big deal. But worth changing if possible

// Could also just zero the stack when convenient

// TODO: Exception checking

// TODO: Function application
// TODO: Type checking
// TODO: Conditionals

// TODO: Serializability. Currently we're encoding function pointers directly in compiled code.

// TODO: We also may need to deal with C++ exceptions. Although we don't use them, we want to be
// able to call out to C++ code that may

// TODO: How to get stack traces out of this?
// Well, we know where the code_offset is at any given time, so we could store that in a register
// or on the stack and use it along with the VM source information to add stack trace info.

// Also, we only really need to store it at spots that can cause errors. Something like
// PUSH_IMMEDIATE will never fail.

// Anatomy of a natively-compiled Scheme function

// Registers:
// rState = r12 = pointer to arete::State*
// rStackI = r13 = during execution, stores stack index.
// after control ends (due to exception or return), stores return value

// Stack:
// 0-8 = pointer to previous native stack frame (if any)
// 8-16 = count of gc'd values
// 16-a = computation stack
// a-b = local values
// b-c = upvalues

// Stack
// rsp+0 = pointer to previous stack frame
// rsp+8 = closure pointer (if any)
// rsp+16 = vmfunction pointer
// rsp+24 = count of gc'd values
// 24 - a = local variables
// a - b = upvalues
// b - c = computation stack

// TODO: Use offsetof and other stuff instead of calling functions to retrieve information
// about a particular State

#include <algorithm>

#include "arete.hpp"

#include "base64.h"

#include "dasm_proto.h"
#include "dasm_x86.h"

#define AR_LOG_JIT(msg) ARETE_LOG((ARETE_LOG_TAG_JIT), "jit", msg)

namespace arete {

DefunGroup jit("jit");

static size_t invocation = 0;
ptrdiff_t print_int(State& state, ptrdiff_t i) {
  std::cout << "print_int(" << invocation++ << "):" << i << std::endl;
  return 0;
}

void print_int_simple(ptrdiff_t i) {
  std::cout << "print_int_simple(" << invocation++ << "):" << i << std::endl;
}

void debug_middle(ptrdiff_t idx, ptrdiff_t i) {
  switch(idx) { 
    case 0: std::cout << "constant ptr: " << i << std::endl;
  }
}

void print_2int_simple(ptrdiff_t a, ptrdiff_t b) {
  std::cout << "print_2int:" << a << ' ' << b << std::endl;
}

void print_something() {
  std::cout << "something" << std::endl;
}

Value state_make_pair(State* state) {
  Value pare(state->make_pair(C_FALSE, C_FALSE));
  std::cout << "state_make_pair returning " << (size_t) pare.bits << std::endl;
  return pare;
}

void state_collect(State* state) {
  std::cout << "COLLECTION BEGINNING " << (ptrdiff_t) state << std::endl;

  state->gc.collect();

  std::cout << "COLLECTION RETURNING" << std::endl;
}

static ptrdiff_t rsp = 0;

Value return_true() {
	return C_TRUE;
}

Value return_true_state(State* state) {
	std::cout << "Return_true called with state " << (ptrdiff_t) state << std::endl;
	return C_TRUE;
}

void return_true_param(ptrdiff_t* thing) {
	(*thing) = 2;
}

ptrdiff_t return_true_int() {
	return 2;
}

void sanity_log_1(ptrdiff_t rsp_) {
  rsp = rsp_;
  std::cout << "sanity_log_1: rsp after stack allocation: " << rsp << std::endl;
}

void sanity_log_2(ptrdiff_t compstack) {
  std::cout << "sanity_log_2: location of computation stack[0]: " << compstack << "(" << (rsp - compstack) << " from rsp)" << std::endl;
}

void sanity_log_3(ptrdiff_t locals) {
  std::cout << "sanity_log_3: location of locals[0]: " << locals << "(" << (rsp - locals) << " from rsp)" << std::endl;
}

void sanity_log_4(ptrdiff_t argc) {
  std::cout << "sanity_log_4: argc: " << argc << std::endl;
}

void sanity_log_5(ptrdiff_t argv) {
  std::cout << "sanity_log_5: argv ptr: " << argv << std::endl;
}

void sanity_log_ret(ptrdiff_t rStackI) {
  std::cout << "sanity_log_ret: rStackI = " << rStackI << std::endl;
}

void sanity_log_end(ptrdiff_t rStackI) {
  std::cout << "sanity_log_end: return value = " << rStackI << std::endl;
}

void print_stack_frame(NativeFrame* frame) {
  std::cout << "<<" << std::endl;;
  std::cout << "stack frame " << (ptrdiff_t) frame << std::endl;
  std::cout << "previous " << (ptrdiff_t) frame->previous << std::endl;
  std::cout << "count " << (ptrdiff_t) frame->value_count << std::endl;
  std::cout << "vmfunction " << (ptrdiff_t) frame->values[0].bits << std::endl;
  std::cout << "closure " << (ptrdiff_t) frame->values[1].bits << std::endl;
  std::cout << "scratch " << (ptrdiff_t) frame->values[2].bits << std::endl;
  for(size_t i = 3; i <= frame->value_count; i++) {
    std::cout << "values[" << i << "]: " << (ptrdiff_t) frame->values[i].bits << std::endl;
  }
  std::cout << ">>" << std::endl;;
}

// Exception handling

// Because string munging in assembly is painful, exceptions are described by the NativeException
// struct and generated through an external C++ procedure

// Exception tag codes
enum {
  E_TYPE,
  E_EVAL
};

// Exception source codes
enum {
  E_ARITY_CHECK,
  E_APPLY_CHECK,
  E_FX_ADD,
  E_FX_SUB,
  E_FX_LT,
};

// Exception message codes
enum {
  E_EXPECTED_FIXNUM_1,
  E_EXPECTED_FIXNUM_2,
};

struct NativeException {
  State* state1;
  State* state;
  Value function;
  size_t code_offset;
  ptrdiff_t tag;
  ptrdiff_t src;
  ptrdiff_t arg1;
  ptrdiff_t arg2;
};

/**
 * Function for generating an exception with a trace from a native function.
 */
Value native_throw_exception(NativeException* exc) {
  std::cout << (ptrdiff_t) exc << std::endl;
  if(!exc) return C_FALSE;

  AR_LOG_JIT("!native_exception called with state " << (ptrdiff_t) exc->state <<
    " VMFunction: " << exc->function.bits << " code_offset: " << exc->code_offset
    << " tag: " << exc->tag << " src: " << exc->src << " arg1: " << exc->arg1
    << " arg2: " << exc->arg2);

  AR_ASSERT(exc->function.type() == VMFUNCTION);

  std::ostringstream os;

  Value stag;

  if(exc->tag == E_TYPE) {
    stag = exc->state->globals[State::S_TYPE_ERROR];
  } else {
    stag = exc->state->globals[State::S_EVAL_ERROR];
  }

  bool check_code = true;
  switch(exc->src) {
    case E_APPLY_CHECK: {
      check_code = false;
      //Value varg1(exc->arg1);
      os << "attempt to apply non-applicable value " << exc->arg1;
      break;
    }
    case E_ARITY_CHECK: {
      check_code = false;
      os << "expected exactly " << exc->arg1 << " arguments but got " << exc->arg2;
      break;
    }
    case E_FX_ADD: os << "primitive fx+ "; break;
    case E_FX_SUB: os << "primitive fx- "; break;
    case E_FX_LT: os << "primitive fx< "; break;
    default: break;
  }

  if(check_code) {
    switch(exc->arg1) {
      case E_EXPECTED_FIXNUM_1: os << "expected argument 1 to be a fixnum"; break;
      case E_EXPECTED_FIXNUM_2: os << "expected argument 2 to be a fixnum"; break;
      default: break;
    }
  }

  exc->state->trace_function(exc->function, 0, exc->code_offset);
  
  return exc->state->make_exception(stag, os.str());
}

// callee-save

// parameters = rdi, rsi, rdx, rcx, r8, r9, xmm0-7
// rsp = stack pointer
// saved = rbx, rbp, rdi, rsi, rsp, r12-15
// rsp-128 is the red zone; can be used for temp values but destroyed by any called function

Value generic_apply(State* state, size_t argc, Value* argv, Value fn) {
  std::cout << "Generic_apply: " << (ptrdiff_t)state << ' ' << (ptrdiff_t) fn.bits <<
    ' ' << (ptrdiff_t) argc << ' ' << (ptrdiff_t) argv << std::endl;


  if(argc > 0) {
    std::cout << "argv[0] " << (ptrdiff_t) argv[0].bits << std::endl;
  }

  Value result(state->apply(fn, argc, argv));

  std::cout << state->get_symbol("print").symbol_value().bits << std::endl;

  return result;
}

struct NativeFrameSize {
  NativeFrame* previous;
  size_t value_count;
  Value closure;
  Value vmfunction;
  Value scratch;
};

static const bool dbg = false;
static const bool sanity_log = true;
static const bool gc_debug = true;

Value vmfunction_to_native(State& state, size_t argc, Value* argv) {
  const char* fn_name = "vmfunction->native";

  AR_FN_EXPECT_TYPE(state, argv, 0, VMFUNCTION);
  Value bv, nfn, vfn;

  vfn = argv[0];

  AR_LOG_JIT("vmfunction_to_native called");

  VMFunction* vmf = vfn.as_unsafe<VMFunction>();

  unsigned local_count = vmf->local_count;
  unsigned stack_max = vmf->stack_max;
  unsigned label = 0;

  // Using the otherwise not-used-in-this codebase kBlah style to denote things that will become
  // constants in the compiled code

  unsigned total_stack = local_count + stack_max;
  unsigned kFrameValues = local_count + stack_max + 2;
  // Locals + computation stack + pointer to VMFunction and Closure

  size_t kFrameSize = sizeof(NativeFrameSize);
  // size_t kValueSize = sizeof(void*);

  // Stack frame offsets
  unsigned kLocalsOffset = kFrameSize;
  //unsigned kUpvaluesOffset = sizeof(NativeFrame) + (local_count * sizeof(void*));
  unsigned kCompStackOffset = kFrameSize + (local_count * sizeof(void*));

  unsigned kTotalStackSize = kFrameSize + (total_stack * 8);
  unsigned kTotalStackSizeAligned = kTotalStackSize;

  if(!((kTotalStackSizeAligned % 16) != 0)) {
    kTotalStackSizeAligned += 8;
  }

  //kTotalStackSizeAligned += 24;

  AR_LOG_JIT("function frame info. stack_max " << stack_max << " kTotalStackSize " << kTotalStackSize << \
    " kTotalStackSizeAligned " << kTotalStackSizeAligned << " kLocalsOffset " << kLocalsOffset << 
    " kCompStackOffset " << kCompStackOffset \
		<< " kFrameValues " << kFrameValues << " kFrameSize " << kFrameSize);

  // A note on a possible source of confusion: in the virtual machine, the stack is a separately
  // allocated data structure from locals and upvalues. Thus there's the computation stack which
  // exists only metaphorically here and is where the values of temporary computation are stored and
  // tracked by the garbage collector, and the actual stack i.e. rsp.

  | .arch x64
  |.section code

  |.define rState, r12
  |.define rStackI, r13
	|.if WINDOWS
	| .define rArg1, rcx
	| .define rArg2, rdx
  | .define rArg3, r8
  | .define rArg4, r9
	|.else
  | .define rArg1, rdi
  | .define rArg2, rsi
  | .define rArg3, rdx
  | .define rArg4, rcx
  | .define rArg5, r8
	|.endif
  |.type state, State, rState

  // Defined separately in case calling conventions changed
  |.define rArgState, rArg1
  |.define rArgArgc, rArg2
  |.define rArgArgv, rArg3
  |.define rArgFn, rArg4

  |.define sFramePrev, [rsp]
  |.define sFrameCount, [rsp+8]
  |.define sFrameFunction, [rsp+16]
  |.define sFrameClosure, [rsp+24]
  |.define sFrameScratch, [rsp+32]

  // Call a simple FUN, for debugging only.
  |.macro simpCall, fun
	| .if WINDOWS
	| push rbp; mov rbp, rsp; mov64 rax, (ptrdiff_t) fun; call rax; mov rsp, rbp; pop rbp
	| .else
  | push rbp; push rsp; mov64 rax, (ptrdiff_t) fun; call rax; pop rsp; pop rbp;
	| .endif
  |.endmacro

  // Call a function which takes arete::State as its first argument
  // and returns a Value
	|.macro stateCallRetValue, method
	// On Windows, a Value is returned as a pointer to a stack value. Can't find
	// documentation for this anywhere, but both Clang and MSVC do it; which
	// means a little dance is needed to get it into rax.
  | .if WINDOWS
  |  push rbp
  |  mov rbp, rsp
  |  mov rArg1, rState
  |  mov64 rax, (ptrdiff_t) method
	|  sub rsp, 32
  |  call rax
	|  add rsp, 32
	|  mov rArg1, [rax]
	|  mov rax, rArg1
	|  mov rsp, rbp
  |  pop rbp
  | .else
  |  push rbp
  |  push rsp
  |  mov rArg1, rState
  |  mov64 rax, (ptrdiff_t) method
  |  call rax
  |  pop rsp
  |  pop rbp
  | .endif
	|.endmacro

  // Call a function which takes arete::State as its first argument
  |.macro stateCall, method
  | .if WINDOWS
  |  push rbp
  |  mov rbp, rsp
  |  mov rArg1, rState
  |  mov64 rax, (ptrdiff_t) method
	|  sub rsp, 32
  |  call rax
	|  add rsp, 32
	|  mov rsp, rbp
  |  pop rbp
  | .else
  |  push rbp
  |  push rsp
  |  mov rArg1, rState
  |  mov64 rax, (ptrdiff_t) method
  |  call rax
  |  pop rsp
  |  pop rbp
  | .endif
  |.endmacro

  // Check that value in STORE is a valid fixnum, if not, generate a type error with SRC and CODE
  // Generates a label
  |.macro checkFixnumP, store, src, code
  | mov rax, store
  | and rax, 1
  | cmp rax, 1
  | jz >1
  // Test failed. Set up registers for exception
  | throwNativeException (code_offset-1), E_TYPE, src, code, 0
  |1:
  |.endmacro

  // jmp to label >1 if value in STORE is immediate
  |.macro immediateJmpTo1, store
  | mov rax, store
  // bits == 0 is #f
  | cmp rax, 0
  | je >1
  | mov rax, store
  | and rax, 3
  | cmp rax, 0
  | jne >1
  |.endmacro

  // Check whether a value in STORE is an exception and return it if so
  |.macro checkExceptionP, store
  | immediateJmpTo1 store
  // this is a valid pointer, extract type and check for exception type & exception active bit
  | mov rax, [store]
  | and rax, 255
  | cmp rax, EXCEPTION
  // not exception type, continue
  | jne >1
  // check exception bit
  | mov rax, [store]
  | and rax, Value::EXCEPTION_ACTIVE_BIT
  | cmp rax, Value::EXCEPTION_ACTIVE_BIT
  | jne >1
  // this is an exception, return immediately
  | mov r14, code[current_insn]
  | mov rStackI, store
  | jmp ->trace_exception
  |1:
  |.endmacro

  // Check whether a value in STORE is applicable and extract its procedure address if so
  |.macro extractProcedureAddr, store
  | immediateJmpTo1 store
  | mov rax, [store]
  | and rax, Value::VALUE_PROCEDURE_BIT
  | cmp rax, Value::VALUE_PROCEDURE_BIT
  | jne >1
  | jmp >2
  | 1:
  | throwNativeException current_insn, E_EVAL, E_APPLY_CHECK, store, 0
  | 2:
  | mov rax, store
  | add rax, (&((Procedure*)0)->procedure_addr)
  | mov rax, [rax]
  |.endmacro

  |.macro popStackTop, store
  | sub rStackI, 8
  | mov store, rStackI
  | mov store, [store]
  |.endmacro

  |.macro getStackTop, store
  | mov store, rStackI
  | sub store, 8
  | mov store, [store]
  |.endmacro

  |.macro throwNativeException, offset, tag, src, arg1, arg2
  // See also ->exception2:
  | sub rsp, 48
  | mov qword [rsp+8], offset
  | mov qword [rsp+16], tag
  | mov qword [rsp+24], src
  | mov qword [rsp+32], arg1
  | mov qword [rsp+40], arg2
  | jmp ->exception
  |.endmacro

  // store a constant in a register
  |.macro storeConstant, reg, idx
  | mov rArg2, [rsp+16]
  // Get &VMFunction::constants
  | add rArg2, (ptrdiff_t)(&((VMFunction*)0)->constants)
  // Get &VMFunction->constants->data[idx]
  | mov rArg2, [rArg2]
  // Get VMFunction->constants->data[idx]
  | add rArg2, (ptrdiff_t)((((ptrdiff_t)&((VectorStorage*)0)->data)) + (idx * 8))
  | mov reg, [rArg2]
  |.endmacro

  |.macro incStack
  | add rStackI, 8
  |.endmacro

  |.macro decStack
  | sub rStackI, 8
  |.endmacro

  |.macro pushConstant, idx
  | storeConstant rArg2, idx
  | mov [rStackI], rArg2
  | incStack
  |.endmacro

  dasm_State *d;
  dasm_init(&d, DASM_MAXSECTION);

  |.actionlist actions
  |.globals globals
  |.globalnames globalnames

  void* globals[globals_MAX == 0 ? 1 : globals_MAX];
  (void) globalnames; // suppress compiler warning

  dasm_setupglobal(&d, globals, globals_MAX);
  dasm_setup(&d, actions);

  dasm_State** Dst = &d;

  |.code

  // Prologue
  // Save registers
	| push rbp
  | mov rbp, rsp
  | push rbx
  | push rState
  | mov rState, rArg1
  | push rStackI

  | push r14; push r15

  // NOTE: At the boundary of any CALL, stack size must be a multiple of 16!
  // PUSHing grows it by only 8.

  | mov r14, rArgArgc
  | mov r15, rArgArgv

  // Allocate stack space
  | sub rsp, kTotalStackSizeAligned

  // Get stack pointer
  | mov rStackI, rsp
  | add rStackI, kCompStackOffset

  // NativeFrame.values[0] = vfn
  | mov qword sFrameFunction, rArgFn
  // NativeFrame.values[1] = closure
  | mov qword sFrameClosure, 18
  | mov qword sFrameScratch, 18

  // NativeFrame.previous = state->gc.native_frames
  | mov rArg1, rState
  | add rArg1, (ptrdiff_t)(&((arete::State*)0)->gc.native_frames)
  | mov rArg2, [rArg1]
  | mov sFramePrev, rArg2
  | mov [rArg1], rsp

  // NativeFrame.value_count
  | mov qword sFrameCount, kFrameValues

  if(dbg && sanity_log) {
    | mov rArg1, rsp
    | simpCall sanity_log_1

    | mov rArg1, rStackI
    | simpCall sanity_log_2

    | mov rArg1, rsp
    | add rArg1, kLocalsOffset
    | simpCall sanity_log_3

    | mov rArg1, r14
    | simpCall sanity_log_4

    | mov rArg1, r15
    | simpCall sanity_log_5
  }

  // Zero out GC frame
  // Assumes function has a stack size of at least one.

  | mov rArg1, rsp
  | add rArg1, kFrameSize

  | mov rArg2, rsp
  | add rArg2, kTotalStackSizeAligned

  | ->gc_zero:
  | mov qword [rArg1], 0
  | add rArg1, 8
  | cmp rArg1, rArg2
  | jne ->gc_zero

  if(dbg) {
    | mov rArg1, rsp
    | simpCall print_stack_frame
  }

  // Execute function body
  size_t code_offset = 0;
  size_t current_insn = 0;
  size_t *code = vmf->code_pointer();
  bool done = false;

  std::unordered_map<size_t, unsigned> offsets_to_labels;

  // Generate all labels

  // Generate labels
  while(!done) {
    size_t loc = 0;

    //std::cout << code_offset << std::endl;
    //std::cout << code[code_offset] << std::endl;
    switch(code[code_offset++]) {
      case OP_FX_ADD: case OP_FX_SUB:
      case OP_FX_LT:
      case OP_POP:
      case OP_ARGV_REST: continue;

      case OP_APPLY:
      case OP_APPLY_TAIL:
      case OP_PUSH_IMMEDIATE:
      case OP_GLOBAL_GET:
      case OP_UPVALUE_GET:
      case OP_UPVALUE_SET:
      case OP_LOCAL_GET:
      case OP_LOCAL_SET:
      case OP_ARGC_EQ:
      case OP_ARGC_GTE:
      case OP_PUSH_CONSTANT: code_offset++; continue;

      case OP_GLOBAL_SET: code_offset += 2; continue;

      case OP_RETURN: continue;
      case OP_RETURN_END: {
        done = true;
        continue;
      }
      case OP_JUMP:
      case OP_JUMP_WHEN:
      case OP_JUMP_UNLESS:
      case OP_JUMP_WHEN_POP: {
        loc = code[code_offset++];
        break;
      }
      default: {
        AR_LOG_JIT("label generation unknown bytecode " << code[code_offset-1]);
        return state.eval_error("unknown bytecode during label generation");
      }
    }

    if(loc > 0){
      auto it = offsets_to_labels.find(loc);
      if(it == offsets_to_labels.end()) {
        unsigned lbl = label++;
        AR_LOG_JIT("code offset " << loc << " => label " << lbl);
        offsets_to_labels.insert(std::make_pair(loc, lbl));
      } else {
        AR_LOG_JIT("code offset " << loc << " == label " << it->second);
      }
    }
  }

  std::vector<size_t> offsets;
  for(auto it : offsets_to_labels) {
    offsets.push_back(it.first);
  }

  std::sort(offsets.begin(), offsets.end(), std::greater<size_t>());

  dasm_growpc(&d, label);

  done = false;
  code_offset = 0;

  while(!done) {
    // Test for label generation
    if(offsets.size() > 0 && code_offset == offsets[offsets.size() - 1]) {
      auto i = offsets_to_labels.find(code_offset);
      unsigned lbl = i->second;
      offsets.pop_back();
      AR_LOG_JIT("generating label " << lbl << " for code offset " << code_offset);
      |=>lbl:
    }

    current_insn = code_offset;

    switch(code[code_offset++]) {
      case OP_PUSH_IMMEDIATE: {
        size_t value = code[code_offset++];
        AR_LOG_JIT("push-immediate " << Value(value) << " (" << (ptrdiff_t) value << ")");
        | mov qword [rStackI], (size_t) value
        | add rStackI, 8
        break;
      }

      case OP_PUSH_CONSTANT: {
        size_t idx = code[code_offset++];
        AR_LOG_JIT("push-constant " << idx);

        | pushConstant idx
        // taking rArg1
        break;
      }

      case OP_POP: {
        AR_LOG_JIT("pop");
        | sub rStackI, 8
        break;
      }

      // TODO undefined
      case OP_GLOBAL_GET: {
        size_t idx = code[code_offset++];

        AR_LOG_JIT("global-get " << idx);

        | storeConstant rArg1, idx
        | add rArg1, (ptrdiff_t)(&(((Symbol*)0)->value))
        | mov rArg1, [rArg1]

        | mov [rStackI], rArg1
        | incStack

        break;
      }

      case OP_LOCAL_GET: {
        size_t idx = code[code_offset++];
        size_t stack_offset = (kLocalsOffset + (8 * idx));
        AR_LOG_JIT("local-get " << idx << " at stack location " << stack_offset);

        | mov rArg1, rsp
        | add rArg1, stack_offset

        | mov rArg1, [rArg1]

        | mov [rStackI], rArg1
   
        | incStack
        break;
      }

      case OP_LOCAL_SET: {
        size_t idx = code[code_offset++];
        size_t stack_offset = (kLocalsOffset + (8 * idx));
        AR_LOG_JIT("local-set " << idx << " at stack location " << stack_offset);

        | mov rArg1, rsp
        | add rArg1, stack_offset

        | popStackTop rArg2
        | mov [rArg1], rArg2

        break;
      }

      case OP_GLOBAL_SET: {
        size_t error_on_undefined = code[code_offset++];
        size_t idx = code[code_offset++];

        AR_LOG_JIT("global-set " << error_on_undefined << " " << idx);
        | storeConstant rArg1, idx
        | add rArg1, (ptrdiff_t)(&(((Symbol*)0)->value))

        | popStackTop rArg2
        | mov [rArg1], rArg2

        break;
      }

      case OP_APPLY_TAIL: 
      case OP_APPLY: {
        size_t argc = code[code_offset++];

        AR_LOG_JIT("apply " << argc);

        // Get pointer to beginning of what we need on the stack (function)

        // Pointer to function on stack
        | mov rArg4, rStackI
        | sub rArg4, ((argc * 8) + 8)

        // Pointer to argv, if there is one
        if(argc > 0) {
          | mov rArg3, rArg4
          | add rArg3, 8
        }

        | mov rArg2, (size_t)argc
        | mov rArg1, rState

        | mov rArg4, [rArg4]

        | extractProcedureAddr rArg4

        | call rax

        | mov rArg1, rax
        | checkExceptionP rArg1

        // Place result on stack
        | sub rStackI, ((argc * 8) + 8)
        | mov [rStackI], rArg1
        | incStack


        break;
      }

      case OP_JUMP: {
        size_t loc = code[code_offset++];
        AR_LOG_JIT("jump " << loc);

        auto it = offsets_to_labels.find(loc);
        AR_ASSERT(it != offsets_to_labels.end() && "jump label was not generated correctly");
        unsigned lbl = it->second;

        | jmp =>lbl
        break;
      }

      case OP_JUMP_WHEN:
      case OP_JUMP_UNLESS:
      case OP_JUMP_WHEN_POP: {
        size_t loc = code[code_offset++];
        // Get top of stack
        // If it is false, 

        auto it = offsets_to_labels.find(loc);
        AR_ASSERT(it != offsets_to_labels.end() && "jump label was not generated correctly");
        unsigned lbl = it->second;
        AR_LOG_JIT("jump-" << ((code[current_insn] == OP_JUMP_UNLESS) ? "unless" : "when") << ' ' << loc << " (label " << lbl << ")");

        if(code[current_insn] == OP_JUMP_WHEN_POP) {
          | popStackTop rArg1
        } else {
          | getStackTop rArg1
        }

        | cmp rArg1, C_FALSE
        if(code[current_insn] == OP_JUMP_UNLESS) {
          | jne =>lbl
        } else {
          | je =>lbl
        }
        break;
      }

      // Fixnum boolean expressions
      case OP_FX_LT: {
        | decStack

        | mov rArg2, rStackI

        | mov rArg1, rStackI
        | sub rArg1, 8

        | checkFixnumP [rArg2], E_FX_LT, E_EXPECTED_FIXNUM_1
        | checkFixnumP [rArg1], E_FX_LT, E_EXPECTED_FIXNUM_1

        | mov rArg1, [rArg1]
        | mov rArg2, [rArg2]

        | decStack
        | mov qword [rStackI], C_FALSE
        | cmp rArg1, rArg2
        | jl >1
        | jmp >2
        | 1: 
        | mov qword [rStackI], C_TRUE
        | 2:
        | incStack

        break;
      }

      // Fixnum arithmetic operations
      case OP_FX_ADD:
      case OP_FX_SUB: {
        unsigned src = 0;

        switch(code[current_insn]) {
          case OP_FX_ADD: src = E_FX_ADD; AR_LOG_JIT("fx+"); break;
          case OP_FX_SUB: src = E_FX_SUB; AR_LOG_JIT("fx-"); break;
        }

        | sub rStackI, 8
        | mov rArg2, rStackI

        // TODO: Why is the shifting necessary? It should be possible to add or subtract
        // two fixnum values and return them as-is

        // 2 labels: two for each type test
        | checkFixnumP [rArg2], src, E_EXPECTED_FIXNUM_2

        // Extract fixnum argument 1 value
        | shr qword [rArg2], 1

        | mov rArg1, rArg2
        | sub rArg1, 8

        | checkFixnumP [rArg1], src, E_EXPECTED_FIXNUM_1
        // extract fixnum argument 2 value
        | shr qword [rArg1], 1

        | mov rax, [rArg1]
        switch(code[current_insn]) {
          case OP_FX_ADD:
            | add rax, [rArg2]
            break;
          case OP_FX_SUB:
            | sub rax, [rArg2]
            break;
        }

        | shl rax, 1
        | inc rax

        | mov [rArg1], rax

        // set top of stack
        break;
      }

      case OP_RETURN:
      case OP_RETURN_END: {
        AR_LOG_JIT("return");
        | sub rStackI, 8
        | mov rStackI, [rStackI]

        | jmp ->unwind

        if(code[current_insn] == OP_RETURN_END)
          done = true;

        break;
      }

      // Checking function arity

      case OP_ARGC_EQ: {
        size_t fargc = code[code_offset++];

        | mov rax, (size_t) fargc
        | cmp rax, r14
        | je >1
        | throwNativeException code_offset-2, E_EVAL, E_ARITY_CHECK, fargc, r14
        | 1:

        AR_LOG_JIT("argc-eq " << fargc);

        // Initialize local variables
        if(fargc) {
          AR_LOG_JIT("initializing locals with loop to " << kLocalsOffset + (8 * fargc));
          | mov rArg1, rsp
          | mov rArg2, r15
          | add rArg1, (kLocalsOffset)
          | mov rArg3, rArg1
          | add rArg3, (8 * fargc)

          |->local_init:
          // mov argv[i] into rArg4
          | mov rArg4, [rArg2]
          // mov rArg4 into locals[i]
           | mov [rArg1], rArg4
          // Increment and check again
          | add rArg2, 8
          | add rArg1, 8
          | cmp rArg1, rArg3
          | jne ->local_init

        }
        if(dbg && gc_debug) {
          | stateCall state_collect;
        }

        if(dbg) {
          | mov rArg1, rsp
          | simpCall print_stack_frame
        }
        break;
      }

      case OP_ARGC_GTE: {
        code_offset++;
        break;
      }

      case OP_ARGV_REST: {
        break;
      }

      default: {
        AR_LOG_JIT("unknown bytecode " << code[code_offset-1]);
        return state.eval_error("unknown bytecode");
      }
    }
  }

  | jmp ->unwind

  // Attempt to trace exception
  |->trace_exception:

  |jmp ->unwind

  |->exception:
  // Append State* state to end of NativeException
  // Arg1 = Pointer to beginning of NativeException structure

  // Tricky math: We've added space for 5/6 of NativeException in the
  // throwNativeException module. Then we set the first value in NativeException
  // to [rsp+40+24] (VMFunction pointer) and push a state pointer on the end
  | mov rArg1, [rsp+48+16]
  | mov [rsp], rArg1
  | push rState
  | push rState
  | mov rArg1, rsp
  | simpCall native_throw_exception
  | mov rStackI, rax
  | add rsp, 48+16 // Reset stack

  |->unwind: 

  // Protect return value
  if(dbg && gc_debug) {
    | mov sFrameScratch, rStackI
    | stateCall state_collect;
    | mov rStackI, sFrameScratch
  }

  if(dbg && sanity_log) {
    | mov rArg1, rStackI
    | simpCall sanity_log_end
  }

  // state->gc.native_frames = NativeFrame.previous
  | mov rArg1, rState
  | add rArg1, (ptrdiff_t)(&((arete::State*)0)->gc.native_frames)
  | mov rArg2, [rsp]
  | mov [rArg1], rArg2
  
  // Free stack space
  | add rsp, kTotalStackSizeAligned

  // Retrieve return value from rStackI
  | mov rax, rStackI

  // Restore registers
  | pop r15; pop r14
  | pop rStackI
	| pop rState
  | pop rbx
  | mov rsp, rbp
	| pop rbp

  // Create return value
  | ret

  size_t size;

  dasm_link(&d, &size);

  AR_FRAME(state, bv, nfn);

  bv = state.make_bytevector<unsigned char>(size);

  AR_LOG_JIT("function size: " << size << " vs " << (code_offset * 8));

  dasm_encode(&d, bv.bv_data());

  dasm_free(&d);

  AR_LOG_JIT("vmfunction->native done; calling function");

  vfn.as_unsafe<VMFunction>()->native_code = bv.as_unsafe<Bytevector>();
  vfn.heap->set_header_bit(Value::VMFUNCTION_NATIVE_BIT);

  vfn.as_unsafe<VMFunction>()->procedure_addr = (c_closure_t)bv.as_unsafe<Bytevector>()->data;

  return vfn;
}

AR_DEFUN("vmfunction->native!", vmfunction_to_native, 1);

Value fn_native_call(State& state, size_t argc, Value* argv, Value fn) {
  static const char* fn_name = "native-call";
  AR_FN_EXPECT_TYPE(state, argv, 0, VMFUNCTION);

  AR_ASSERT(argv[0].heap->get_header_bit(Value::VMFUNCTION_NATIVE_BIT));

  Value vfn = argv[0];
  AR_FRAME(state, vfn);
  AR_ASSERT(state.gc.live(vfn.as_unsafe<VMFunction>()->native_code));

  Value bv(vfn.as_unsafe<VMFunction>()->native_code);

  if(dbg) {
    std::cout << "Bv size: " << vfn.as_unsafe<VMFunction>()->native_code->size << std::endl;

    std::cout << "VMFunction ptr: " << (ptrdiff_t)vfn.bits << std::endl;
    std::cout << "Constants ref: " << (ptrdiff_t)&vfn.as<VMFunction>()->constants << std::endl;
    std::cout << "Constants ptr: " << (ptrdiff_t)vfn.as<VMFunction>()->constants << std::endl;
    std::cout << "Constants ptr[0]: " << (ptrdiff_t)&vfn.as<VMFunction>()->constants->data[0] << std::endl;

    std::cout << "#(";
    for(size_t i = 0; i != vfn.as<VMFunction>()->constants->length; i++) {
      std::cout << (ptrdiff_t)vfn.as<VMFunction>()->constants->data[i].bits;
      std::cout << ' ';
    }
    std::cout << ')' << std::endl;
    std::cout << "Calling against state: " << (ptrdiff_t) &state << std::endl;

    std::cout << " argc: " << argc-1 << " argv ptr: " << (size_t)argv+8 << std::endl;
  }
  ptrdiff_t (*ptr)(State*, size_t, size_t, size_t) = (ptrdiff_t (*)(State*, size_t, size_t)) vfn.as_unsafe<VMFunction>()->native_code->data;
  ptrdiff_t result = ptr(&state, argc-1, (size_t)argv+8, (size_t)vfn.bits);

  if(dbg) {
    std::cout << "result int: " << (result) << std::endl;
    std::cout << "result: " << Value(result) << std::endl;

    std::cout << "state ptr: " << (ptrdiff_t)&state << std::endl;
    std::cout << "native_Frames: " << (ptrdiff_t)state.gc.native_frames << std::endl;
  }

  AR_ASSERT(state.gc.native_frames == 0);
  return result;
}
AR_DEFUN("native-call", fn_native_call, 1);

void load_native_compiler(State& state) {
  jit.install(state);
}

}
