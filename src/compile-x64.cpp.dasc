// compile-x64.cpp.dasc - Compile bytecode to native amd64 code
// This is a DynASM file. Must be run through DynASM to produce a C++ file

// Further reading
// Agner Fog, Calling Conventions
// http://www.agner.org/optimize/calling_conventions.pdf

// Hint: If it's crashing for no apparent reason, it's probably because the stack
// size is screwed up

// This discusses the Windows struct return issue, but doesn't
// seem to indicate why functions returning Value (only 64 bits) 
// seem to use a different calling convention
// https://msdn.microsoft.com/en-us/library/7572ztz4.aspx

// TODO: GC frames can be trivially omitted in certain cases; if a function
// does not have upvalues, never calls other functions or allocation primitives

// A function like say, memv or assq might benefit heavily from this, but we'd probably need to
// inline recursive calls in the Scheme compiler in order to make it easy to determine whether this
// is possible here.

// TODO: Why use rStackI? Why not just use the RSP and decrement/increment by 8? Would probably
// reduce code quite a bit

// TODO: Currently variables are GC-tracked much in the way we track them C++-side. But in assembly
// we have full control over the stack. Rather than using the NativeFrame structure, for example,
// we could just guarantee that everything from rsp to rsp+whatever is a pointer. One issue with the
// current approach vs VMFrames is that all stack values will be tracked while a function is live, 
// which means some garbage will remain. Imagine something like this being a memory leak:
// (let ()
//   ;; really complex computation that makes a big list
//   ;; main loop of a program that doesn't clear the stack
// )
// In practice, probably not a big deal. But worth changing if possible

// Could also just zero the stack when convenient

// TODO: Exception checking

// TODO: Function application
// TODO: Type checking
// TODO: Conditionals

// TODO: Serializability. Currently we're encoding function pointers directly in compiled code.

// TODO: We also may need to deal with C++ exceptions. Although we don't use them, we want to be
// able to call out to C++ code that may

// TODO: How to get stack traces out of this?
// Well, we know where the code_offset is at any given time, so we could store that in a register
// or on the stack and use it along with the VM source information to add stack trace info.

// Also, we only really need to store it at spots that can cause errors. Something like
// PUSH_IMMEDIATE will never fail.

// Anatomy of a natively-compiled Scheme function

// Registers:
// rState = r12 = pointer to arete::State*
// rStackI = r13 = during execution, stores stack index.
// after control ends (due to exception or return), stores return value

// Stack:
// 0-8 = pointer to previous native stack frame (if any)
// 8-16 = count of gc'd values
// 16-a = computation stack
// a-b = local values
// b-c = upvalues

// TODO: Use offsetof and other stuff instead of calling functions to retrieve information
// about a particular State

#include <algorithm>

#include "arete.hpp"

#include "dasm_proto.h"
#include "dasm_x86.h"

#define AR_LOG_JIT(msg) ARETE_LOG((ARETE_LOG_TAG_JIT), "jit", msg)

namespace arete {

DefunGroup jit("jit");

static size_t invocation = 0;
ptrdiff_t print_int(State& state, ptrdiff_t i) {
  std::cout << "print_int(" << invocation++ << "):" << i << std::endl;
  return 0;
}

void print_int_simple(ptrdiff_t i) {
  std::cout << "print_int_simple(" << invocation++ << "):" << i << std::endl;
}

void debug_middle(ptrdiff_t idx, ptrdiff_t i) {
  switch(idx) { 
    case 0: std::cout << "constant ptr: " << i << std::endl;
  }
}

void print_2int_simple(ptrdiff_t a, ptrdiff_t b) {
  std::cout << "print_2int:" << a << ' ' << b << std::endl;
}

void print_something() {
  std::cout << "something" << std::endl;
}

Value state_make_pair(State* state) {
  Value pare(state->make_pair(C_FALSE, C_FALSE));
  std::cout << "state_make_pair returning " << (size_t) pare.bits << std::endl;
  return pare;
}

void state_collect(State* state) {
  std::cout << "COLLECTION BEGINNING " << (ptrdiff_t) state << std::endl;

  state->gc.collect();

  std::cout << "COLLECTION RETURNING" << std::endl;
}

static ptrdiff_t rsp = 0;

Value return_true() {
	return C_TRUE;
}

Value return_true_state(State* state) {
	std::cout << "Return_true called with state " << (ptrdiff_t) state << std::endl;
	return C_TRUE;
}

void return_true_param(ptrdiff_t* thing) {
	(*thing) = 2;
}

ptrdiff_t return_true_int() {
	return 2;
}

void sanity_log_1(ptrdiff_t rsp_) {
  rsp = rsp_;
  std::cout << "sanity_log_1: rsp after stack allocation: " << rsp << std::endl;
}

void sanity_log_2(ptrdiff_t compstack) {
  std::cout << "sanity_log_2: location of computation stack[0]: " << compstack << "(" << (rsp - compstack) << " from rsp)" << std::endl;
}

void sanity_log_3(ptrdiff_t locals) {
  std::cout << "sanity_log_3: location of locals[0]: " << locals << "(" << (rsp - locals) << " from rsp)" << std::endl;
}

void sanity_log_4(ptrdiff_t argc) {
  std::cout << "sanity_log_4: argc: " << argc << std::endl;
}

void sanity_log_5(ptrdiff_t argv) {
  std::cout << "sanity_log_5: argv ptr: " << argv << std::endl;
}

void sanity_log_end(ptrdiff_t rStackI) {
  std::cout << "sanity_log_end: rStackI = " << rStackI << std::endl;
}

// Exception handling

// Because string munging in assembly is painful, exceptions are described by the NativeException
// struct and generated through an external C++ procedure

// Exception tag codes
enum {
  E_TYPE,
  E_EVAL
};

// Exception source codes
enum {
  E_ARITY_CHECK,
  E_FX_ADD
};

// Exception message codes
enum {
  E_EXPECTED_FIXNUM_1,
  E_EXPECTED_FIXNUM_2,
};

struct NativeException {
  State* state1;
  State* state;
  Value function;
  size_t code_offset;
  ptrdiff_t tag;
  ptrdiff_t src;
  ptrdiff_t arg1;
  ptrdiff_t arg2;
};

/**
 * Function for generating an exception with a trace from a native function.
 */
Value native_throw_exception(NativeException* exc) {
  std::cout << (ptrdiff_t) exc << std::endl;
  if(!exc) return C_FALSE;

  AR_LOG_JIT("!native_exception called with state " << (ptrdiff_t) exc->state <<
    " VMFunction: " << exc->function.bits << " code_offset: " << exc->code_offset
    << " tag: " << exc->tag << " src: " << exc->src << " arg1: " << exc->arg1
    << " arg2: " << exc->arg2);

  AR_ASSERT(exc->function.type() == VMFUNCTION);

  std::ostringstream os;

  Value stag;

  if(exc->tag == E_TYPE) {
    stag = exc->state->globals[State::S_TYPE_ERROR];
  } else {
    stag = exc->state->globals[State::S_EVAL_ERROR];
  }

  bool check_code = true;
  switch(exc->src) {
    case E_ARITY_CHECK: {
      check_code = false;
      os << "expected exactly " << exc->arg1 << " arguments but got " << exc->arg2;
      break;
    }
    case E_FX_ADD: {
      os << "primitive fx+ ";
      break;
    }
    default: break;
  }

  if(check_code) {
    switch(exc->arg1) {
      case E_EXPECTED_FIXNUM_1: os << "expected argument 1 to be a fixnum"; break;
      case E_EXPECTED_FIXNUM_2: os << "expected argument 2 to be a fixnum"; break;
      default: break;
    }
  }

  exc->state->trace_function(exc->function, 0, exc->code_offset);
  
  return exc->state->make_exception(stag, os.str());
}


// callee-save

// parameters = rdi, rsi, rdx, rcx, r8, r9, xmm0-7
// rsp = stack pointer
// saved = rbx, rbp, rdi, rsi, rsp, r12-15
// rsp-128 is the red zone; can be used for temp values but destroyed by any called function

Value generic_apply(State* state, Value fn, size_t argc, Value* argv) {
  std::cout << "Generic_apply: " << (ptrdiff_t)state << ' ' << (ptrdiff_t) fn.bits <<
    ' ' << (ptrdiff_t) argc << ' ' << (ptrdiff_t) argv << std::endl;


  if(argc > 0) {
    std::cout << "argv[0] " << (ptrdiff_t) argv[0].bits << std::endl;
  }

  Value result(state->apply(fn, argc, argv));

  std::cout << state->get_symbol("print").symbol_value().bits << std::endl;

  return result;
}

Value vmfunction_to_native(State& state, size_t argc, Value* argv) {
  static const bool sanity_log = true;
  const char* fn_name = "vmfunction->native";

  AR_FN_EXPECT_TYPE(state, argv, 0, VMFUNCTION);
  Value bv, nfn, vfn;

  vfn = argv[0];

  AR_LOG_JIT("vmfunction_to_native called");

  VMFunction* vmf = vfn.as_unsafe<VMFunction>();

  unsigned local_count = vmf->local_count;
  unsigned stack_max = vmf->stack_max;
  unsigned label = 0;

  // Using the otherwise not-used-in-this codebase kBlah style to denote things that will become
  // constants in the compiled code

  unsigned total_stack = local_count+stack_max;
  unsigned kFrameSize = local_count + stack_max + 1;
  // Locals + computation stack + pointer to VMFunction
  unsigned kTotalStackSize = (sizeof(NativeFrame) - sizeof(void*)) + ((total_stack) * sizeof(void*));
  size_t kValueSize = sizeof(void*);
  unsigned kLocalsOffset = sizeof(NativeFrame);
  unsigned kCompStackOffset = sizeof(NativeFrame) + (local_count * sizeof(void*));
  //unsigned kCompStackOffset = 24; //kTotalStackSize - (kValueSize * stack_max);
  unsigned kTotalStackSizeAligned = kTotalStackSize;

  if((kFrameSize % 2) == 0) {
    // TODO this may need to be different on Windows
    std::cout << "Adjusting stack size to be multiple of 16" << std::endl;
    kTotalStackSizeAligned += 8;
  }
  kTotalStackSizeAligned += 8;

  AR_LOG_JIT("function frame info. stack_max " << stack_max << " kTotalStackSize " << kTotalStackSize << \
    " kTotalStackSizeAligned " << kTotalStackSizeAligned << " kLocalsOffset " << kLocalsOffset << 
    " kCompStackOffset " << kCompStackOffset \
		<< " kFrameSize " << kFrameSize);

  // A note on a possible source of confusion: in the virtual machine, the stack is a separately
  // allocated data structure from locals and upvalues. Thus there's the computation stack which
  // exists only metaphorically here and is where the values of temporary computation are stored and
  // tracked by the garbage collector, and the actual stack i.e. rsp.

  | .arch x64
  |.section code

  |.define rState, r12
  |.define rStackI, r13
	|.if WINDOWS
	| .define rArg1, rcx
	| .define rArg2, rdx
  | .define rArg3, r8
  | .define rArg4, r9
	|.else
  | .define rArg1, rdi
  | .define rArg2, rsi
  | .define rArg3, rdx
  | .define rArg4, rcx
  | .define rArg5, r8
	|.endif
  |.type state, State, rState

  // Defined separately in case calling conventions changed
  |.define rArgState, rArg1
  |.define rArgArgc, rArg2
  |.define rArgArgv, rArg3
  |.define rArgFn, rArg4

  |.define sFramePrev, [rsp]
  |.define sFrameCount, [rsp+8]

  // Call a simple FUN, for debugging only.
  |.macro simpCall, fun
	| .if WINDOWS
	| push rbp; mov rbp, rsp; mov64 rax, (ptrdiff_t) fun; call rax; mov rsp, rbp; pop rbp
	| .else
  | push rbp; push rsp; mov64 rax, (ptrdiff_t) fun; call rax; pop rsp; pop rbp;
	| .endif
  |.endmacro

  // Call a function which takes arete::State as its first argument
  // and returns a Value
	|.macro stateCallRetValue, method
	// On Windows, a Value is returned as a pointer to a stack value. Can't find
	// documentation for this anywhere, but both Clang and MSVC do it; which
	// means a little dance is needed to get it into rax.
  | .if WINDOWS
  |  push rbp
  |  mov rbp, rsp
  |  mov rArg1, rState
  |  mov64 rax, (ptrdiff_t) method
	|  sub rsp, 32
  |  call rax
	|  add rsp, 32
	|  mov rArg1, [rax]
	|  mov rax, rArg1
	|  mov rsp, rbp
  |  pop rbp
  | .else
  |  push rbp
  |  push rsp
  |  mov rArg1, rState
  |  mov64 rax, (ptrdiff_t) method
  |  call rax
  |  pop rsp
  |  pop rbp
  | .endif
	|.endmacro

  // Call a function which takes arete::State as its first argument
  |.macro stateCall, method
  | .if WINDOWS
  |  push rbp
  |  mov rbp, rsp
  |  mov rArg1, rState
  |  mov64 rax, (ptrdiff_t) method
	|  sub rsp, 32
  |  call rax
	|  add rsp, 32
	|  mov rsp, rbp
  |  pop rbp
  | .else
  |  push rbp
  |  push rsp
  |  mov rArg1, rState
  |  mov64 rax, (ptrdiff_t) method
  |  call rax
  |  pop rsp
  |  pop rbp
  | .endif
  |.endmacro

  // Check that value in STORE is a valid fixnum, if not, generate a type error with SRC and CODE
  // Generates a label
  |.macro checkFixnumP, store, src, code
  | mov rax, store
  | and rax, 1
  | cmp rax, 1
  | jz >1
  // Test failed. Set up registers for exception
  | throwNativeException (code_offset-1), E_TYPE, src, code, 0
  |1:
  |.endmacro

  // Check that value in STORE matches TYPE, if not, generate a type error with SRC and CODE
  |.macro checkHeapType, store, type, src, code
  | mov rax, store
  | and rax, 1
  | cmp rax, 0
  |.endmacro

  // takes rStackI and sets STORE to the rsp offset it needs
  |.macro getCompStackIndex, store
  | mov store, rStackI
  | imul store, kValueSize
  | add store, kCompStackOffset
  | add store, rsp
  |.endmacro

  // Put a pointer to stack[STORE] in store, assuming STORE
  |.macro getCompStackIndexAt, store
  | imul store, kValueSize
  | add store, kCompStackOffset
  | add store, rsp
  |.endmacro

  |.macro getCompStackTopValue, store
  | mov store, rStackI
  | dec store
  | imul store, kValueSize
  | add store, kCompStackOffset
  | add store, rsp
  | mov store, [store]
  |.endmacro

  |.macro throwNativeException, offset, tag, src, arg1, arg2
  // See also ->exception2:
  | sub rsp, 48
  | mov qword [rsp+8], offset
  | mov qword [rsp+16], tag
  | mov qword [rsp+24], src
  | mov qword [rsp+32], arg1
  | mov qword [rsp+40], arg2
  | jmp ->exception
  |.endmacro

  // store a constant in a register
  |.macro storeConstant, reg, idx
  | mov rArg2, [rsp+16]
  // Get &VMFunction::constants
  | add rArg2, (ptrdiff_t)(&((VMFunction*)0)->constants)
  // Get &VMFunction->constants->data[idx]
  | mov rArg2, [rArg2]
  // Get VMFunction->constants->data[idx]
  | add rArg2, (ptrdiff_t)((((ptrdiff_t)&((VectorStorage*)0)->data)) + (idx * 8))
  | mov reg, [rArg2]
  |.endmacro


  |.macro pushConstant, idx
  | storeConstant rArg2, idx
  // Push a value from the constants array onto the stack
  | getCompStackIndex rArg1
  | mov [rArg1], rArg2
  //| mov qword [rArg1], 821

  | inc rStackI
  |.endmacro
  dasm_State *d;
  dasm_init(&d, DASM_MAXSECTION);

  |.actionlist actions
  |.globals globals
  |.globalnames globalnames

  void* globals[globals_MAX == 0 ? 1 : globals_MAX];
  (void) globalnames; // suppress compiler warning

  dasm_setupglobal(&d, globals, globals_MAX);
  dasm_setup(&d, actions);

  dasm_State** Dst = &d;

  |.code

  // Prologue
  // Save registers
	| push rbp
  | mov rbp, rsp
  | push rbx
  | push rState
  | mov rState, rArg1
  | push rStackI

  | push r14; push r15

  // NOTE: At the boundary of any CALL, stack size must be a multiple of 16!
  // PUSHing grows it by only 8.

  | mov r14, rArgArgc
  | mov r15, rArgArgv

  | mov rStackI, 0

  // Allocate stack space
  | sub rsp, kTotalStackSizeAligned

  // NativeFrame.values[0] = vfn
  | mov qword [rsp+16], rArgFn

  // NativeFrame.previous = state->gc.native_frames
  | mov rArg1, rState
  | add rArg1, (ptrdiff_t)(&((arete::State*)0)->gc.native_frames)
  | mov rArg2, [rArg1]
  | mov sFramePrev, rArg2
  | mov [rArg1], rsp

  // NativeFrame.value_count
  | mov qword sFrameCount, kFrameSize

  if(sanity_log) {
    | mov rArg1, rsp
    | simpCall sanity_log_1

    | getCompStackIndex rArg1
    | simpCall sanity_log_2

    | mov rArg1, rsp
    | add rArg1, kLocalsOffset
    | simpCall sanity_log_3

    | mov rArg1, r14
    | simpCall sanity_log_4

    | mov rArg1, r15
    | simpCall sanity_log_5
  }

  // Zero out GC frame
  // Assumes function has a stack size of at least one.

  // rArg2 = loop limit, kFrameSize + rsp
  | mov rArg2, rsp
  | add rArg2, ((kFrameSize*8)+8)

  // Set rArg1 to NativeFrame::values[0]
  | mov rArg1, rsp
  | add rArg1, 16

  |->gc_zero:
  // Now add 8 to rArg1 at top of loop so it points at NativeFrame::values[0]
  | add rArg1, 8
  // Set value to zero
  | mov qword [rArg1], 0
  | cmp rArg1, rArg2
  | jne ->gc_zero

/*
  | mov rArg2, rsp; 
	| stateCall state_collect;
  */

  // Execute function body
  size_t code_offset = 0;
  size_t *code = vmf->code_pointer();
  bool done = false;

  std::unordered_map<size_t, unsigned> offsets_to_labels;

  // Generate all labels

  // Generate labels
  while(!done) {
    size_t loc = 0;

    //std::cout << code_offset << std::endl;
    //std::cout << code[code_offset] << std::endl;
    switch(code[code_offset++]) {

      case OP_LOCAL_GET_0:
      case OP_FX_ADD: 
      case OP_POP:
      case OP_ARGV_REST: continue;

      case OP_APPLY:
      case OP_APPLY_TAIL:
      case OP_PUSH_IMMEDIATE:
      case OP_GLOBAL_GET:
      case OP_UPVALUE_GET:
      case OP_UPVALUE_SET:
      case OP_LOCAL_GET:
      case OP_LOCAL_SET:
      case OP_ARGC_EQ:
      case OP_ARGC_GTE:
      case OP_PUSH_CONSTANT: code_offset++; continue;

      case OP_GLOBAL_SET: code_offset += 2; continue;

      case OP_RETURN: continue;
      case OP_RETURN_END: {
        done = true;
        continue;
      }
      case OP_JUMP:
      case OP_JUMP_WHEN:
      case OP_JUMP_UNLESS:
      case OP_JUMP_WHEN_POP: {
        loc = code[code_offset++];
        break;
      }
      default: {
        AR_LOG_JIT("label generation unknown bytecode " << code[code_offset-1]);
        return state.eval_error("unknown bytecode during label generation");
      }
    }

    if(loc > 0){
      auto it = offsets_to_labels.find(loc);
      if(it == offsets_to_labels.end()) {
        unsigned lbl = label++;
        AR_LOG_JIT("code offset " << loc << " => label " << lbl);
        offsets_to_labels.insert(std::make_pair(loc, lbl));
      } else {
        AR_LOG_JIT("code offset " << loc << " == label " << it->second);
      }
    }
  }

  std::vector<size_t> offsets;
  for(auto it : offsets_to_labels) {
    offsets.push_back(it.first);
  }

  std::sort(offsets.begin(), offsets.end(), std::greater<size_t>());

  dasm_growpc(&d, label);

  done = false;
  code_offset = 0;

  while(!done) {
    if(offsets.size() > 0 && code_offset == offsets[offsets.size() - 1]) {
      auto i = offsets_to_labels.find(code_offset);
      unsigned lbl = i->second;
      offsets.pop_back();
      AR_LOG_JIT("generating label " << lbl << " for code offset " << code_offset);
      |=>lbl:
    }

    // Implementation of conditionals

    // JUMP jumps to an offset in the bytecode. So, we'll store all the bytecode offsets to jump
    // to in some kind of data structure,
    
    // and regen them.

    // Generate labels here

    switch(code[code_offset++]) {
      case OP_PUSH_IMMEDIATE: {
        size_t value = code[code_offset++];
        AR_LOG_JIT("push-immediate " << Value(value) << " (" << (ptrdiff_t) value << ")");
        // Retrieve location of stack value
        | getCompStackIndex rArg1
        // Set value
        | mov qword [rArg1], (size_t) value
        // Increment stack
        | inc rStackI
        break;
      }

      case OP_PUSH_CONSTANT: {
        size_t idx = code[code_offset++];
        AR_LOG_JIT("push-constant " << idx);

        | pushConstant idx
        // taking rArg1
        break;
      }

      case OP_POP: {
        AR_LOG_JIT("pop");
        | dec rStackI
        break;
      }

      // TODO undefined
      case OP_GLOBAL_GET: {
        size_t idx = code[code_offset++];

        AR_LOG_JIT("global-get " << idx);

        | storeConstant rArg1, idx
        | add rArg1, (ptrdiff_t)(&(((Symbol*)0)->value))
        | mov rArg1, [rArg1]

        | getCompStackIndex rArg2
        | mov [rArg2], rArg1
        | inc rStackI

        break;
      }

      case OP_LOCAL_GET_0: {
        AR_LOG_JIT("local-get-0 from stack location " << kLocalsOffset);

        | mov rArg1, rsp
        | add rArg1, kLocalsOffset

        | mov rArg1, [rArg1]

        | getCompStackIndex rArg2
        | mov [rArg2], rArg1

        | inc rStackI
        break;
      }

      case OP_GLOBAL_SET: {
        size_t error_on_undefined = code[code_offset++];
        size_t idx = code[code_offset++];

        AR_LOG_JIT("global-set " << error_on_undefined << " " << idx);
        | storeConstant rArg1, idx
        | add rArg1, (ptrdiff_t)(&(((Symbol*)0)->value))

        | dec rStackI
        | getCompStackIndex rArg2
        | mov rArg2, [rArg2]
        | mov [rArg1], rArg2

        break;
      }

      case OP_APPLY_TAIL: 
      case OP_APPLY: {
        size_t argc = code[code_offset++];

        AR_LOG_JIT("apply " << argc);

        // Get pointer to beginning of what we need on the stack (function)
        | mov rArg2, rStackI
        | sub rArg2, (argc + 1)
        | getCompStackIndexAt rArg2

        // Get pointer to argv (after function on stack)
        | mov rArg4, rArg2
        | add rArg4, 8
        // Load function pointer
        | mov rArg2, [rArg2]

        | mov rArg1, rState
        | mov rArg3, (size_t) argc

        | mov64 rax, (size_t) generic_apply
        | call rax

        // TODO: Check exception

        // Place result on stack
        | sub rStackI, argc

        | getCompStackIndex rArg1
        | mov [rArg1], rax
        | inc rStackI

        break;
      }

      case OP_JUMP: {
        size_t loc = code[code_offset++];
        AR_LOG_JIT("jump " << loc);

        auto it = offsets_to_labels.find(loc);
        AR_ASSERT(it != offsets_to_labels.end() && "jump label was not generated correctly");
        unsigned lbl = it->second;

        | jmp =>lbl
        break;
      }

      case OP_JUMP_WHEN:
      case OP_JUMP_UNLESS:
      case OP_JUMP_WHEN_POP: {
        size_t loc = code[code_offset++];
        // Get top of stack
        // If it is false, 

        auto it = offsets_to_labels.find(loc);
        AR_ASSERT(it != offsets_to_labels.end() && "jump label was not generated correctly");
        unsigned lbl = it->second;
        AR_LOG_JIT("jump-" << ((code[code_offset-2] == OP_JUMP_UNLESS) ? "unless" : "when") << loc << " (label " << lbl << ")");

        | getCompStackTopValue rArg1
        if(code[code_offset-2] == OP_JUMP_WHEN_POP) {
          | dec rStackI
        }
        | cmp rArg1, C_FALSE
        if(code[code_offset-2] == OP_JUMP_UNLESS) {
          | jne =>lbl
        } else {
          | je =>lbl
        }
        break;
      }

      case OP_FX_ADD: {
        | getCompStackIndex rArg2
        AR_LOG_JIT("fx+");
        | sub rArg2, 8

        // 2 labels: two for each type test
        | checkFixnumP [rArg2], E_FX_ADD, E_EXPECTED_FIXNUM_2

        // Extract fixnum argument 1 value
        | shr qword [rArg2], 1

        | getCompStackIndex rArg1
        | sub rArg1, 16
        | checkFixnumP [rArg1], E_FX_ADD, E_EXPECTED_FIXNUM_1
        // extract fixnum argument 2 value
        | shr qword [rArg1], 1

        | mov rax, [rArg1]
        | add rax, [rArg2]

        | shl rax, 1
        | inc rax

        | mov [rArg1], rax
        | dec rStackI
        break;
      }

      case OP_RETURN:
      case OP_RETURN_END: {
        AR_LOG_JIT("return");
        //| getCompStackTop rax
        //| mov rArg2, [rsp+16]
        //| mov rArg2, rax
        //| stateCall print_int
        //| sub rStackI, 1

        | sub rStackI, 1
        | getCompStackIndex rArg1
        | mov rStackI, [rArg1]
        | jmp ->unwind

        if(code[code_offset-1] == OP_RETURN_END)
          done = true;

        break;
      }

      // Checking function arity

      case OP_ARGC_EQ: {
        size_t fargc = code[code_offset++];

        | mov rax, (size_t) fargc
        | cmp rax, r14
        | je >1
        | throwNativeException code_offset-2, E_EVAL, E_ARITY_CHECK, fargc, r14
        | 1:

        AR_LOG_JIT("argc-eq " << fargc);

        // Initialize local variables
        if(fargc) {
          AR_LOG_JIT("initializing locals with loop to " << kLocalsOffset + (8 * fargc));
          | mov rArg1, rsp
          | mov rArg2, r15
          | add rArg1, (kLocalsOffset)
          | mov rArg3, rArg1
          | add rArg3, (8 * fargc)

          |->local_init:
          // mov argv[i] into rArg4
          | mov rArg4, [rArg2]
          // mov rArg4 into locals[i]
           | mov [rArg1], rArg4
          // Increment and check again
          | add rArg2, 8
          | add rArg1, 8
          | cmp rArg1, rArg3
          | jne ->local_init

        }

        break;
      }

      case OP_ARGC_GTE: {
        code_offset++;
        break;
      }

      case OP_ARGV_REST: {
        break;
      }

      default: {
        AR_LOG_JIT("unknown bytecode " << code[code_offset-1]);
        return state.eval_error("unknown bytecode");
      }
    }
  }

  | jmp ->unwind

  |->exception:
  // Append State* state to end of NativeException
  // Arg1 = Pointer to beginning of NativeException structure

  // Tricky math: We've added space for 5/6 of NativeException in the
  // throwNativeException module. Then we set the first value in NativeException
  // to [rsp+40+24] (VMFunction pointer) and push a state pointer on the end
  | mov rArg1, [rsp+48+16]
  | mov [rsp], rArg1
  | push rState
  | push rState
  | mov rArg1, rsp
  | simpCall native_throw_exception
  | mov rStackI, rax
  | add rsp, 48+16 // Reset stack

  |->unwind: 

  | mov [rsp+24], rStackI
	| stateCall state_collect;
  | mov rStackI, [rsp+24]

  if(sanity_log) {
    | mov rArg1, rStackI
    | simpCall sanity_log_end
  }

  // state->gc.native_frames = NativeFrame.previous
  | mov rArg1, rState
  | add rArg1, (ptrdiff_t)(&((arete::State*)0)->gc.native_frames)
  | mov rArg2, [rsp]
  | mov [rArg1], rArg2
  
  // Free stack space
  | add rsp, kTotalStackSizeAligned

  // Retrieve return value from rStackI
  | mov rax, rStackI

  // Restore registers
  | pop r15; pop r14
  | pop rStackI
	| pop rState
  | pop rbx
  | mov rsp, rbp
	| pop rbp

  // Create return value
  | ret

  size_t size;

  dasm_link(&d, &size);

  AR_FRAME(state, bv, nfn);

  bv = state.make_bytevector<unsigned char>(size);

  std::cout << "function size: " << size << " vs " << (code_offset * 8) << std::endl;

  dasm_encode(&d, bv.bv_data());

  dasm_free(&d);

  AR_LOG_JIT("vmfunction->native done; calling function");

  ptrdiff_t (*ptr)(State*, size_t, size_t, size_t) = (ptrdiff_t (*)(State*, size_t, size_t)) bv.bv_data();

  std::cout << "VMFunction ptr: " << (ptrdiff_t)vfn.bits << std::endl;
  std::cout << "Constants ref: " << (ptrdiff_t)&vfn.as<VMFunction>()->constants << std::endl;
  std::cout << "Constants ptr: " << (ptrdiff_t)vfn.as<VMFunction>()->constants << std::endl;
  std::cout << "Constants ptr[0]: " << (ptrdiff_t)&vfn.as<VMFunction>()->constants->data[0] << std::endl;

  std::cout << "#(";
  for(size_t i = 0; i != vfn.as<VMFunction>()->constants->length; i++) {
    std::cout << (ptrdiff_t)vfn.as<VMFunction>()->constants->data[i].bits;
    std::cout << ' ';
  }
  std::cout << ')' << std::endl;
	std::cout << "Calling against state: " << (ptrdiff_t) &state << std::endl;

  std::cout << " argc: " << argc-1 << " argv ptr: " << (size_t)argv+8 << std::endl;
  ptrdiff_t result = ptr(&state, argc-1, (size_t)argv+8, (size_t)vfn.bits);

  //std::cout << (ptrdiff_t)state.get_symbol("print").symbol_value().bits << std::endl;
  std::cout << "result int: " << (result) << std::endl;
  std::cout << "result: " << Value(result) << std::endl;

  std::cout << "state ptr: " << (ptrdiff_t)&state << std::endl;
  std::cout << "native_Frames: " << (ptrdiff_t)state.gc.native_frames << std::endl;
  AR_ASSERT(state.gc.native_frames == 0);

  return result;
}

AR_DEFUN("vmfunction->native", vmfunction_to_native, 1);

void load_native_compiler(State& state) {
  jit.install(state);
}

}
