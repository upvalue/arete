// compile-x64.cpp.dasc - Compile bytecode to native amd64 code
// This is a DynASM file. Must be run through DynASM to produce a C++ file

// Major todos

// TODO: Full closure support
// TODO: Allocate off of GC heap

#include <algorithm>
#include <functional>

#include "arete.hpp"

#if ARETE_64_BIT == 0

namespace arete {
void load_native_compiler(State&) {}
}

#else

// Further reading
// Agner Fog, Calling Conventions
// http://www.agner.org/optimize/calling_conventions.pdf

// Hint: If it's crashing for no apparent reason, it's probably because the stack
// size is screwed up

// TODO: Tail call applications, how to do them in an efficent manner?
// Specifically, how to pass and preserve argv?

// TODO: Windows wants us to allocate space on the stack to return structs

// This discusses the Windows struct return issue, but doesn't
// seem to indicate why functions returning Value (only 64 bits) 
// seem to use a different calling convention
// https://msdn.microsoft.com/en-us/library/7572ztz4.aspx

// This seems to be limited to structs with constructors/destructors; a simple struct, even one with a union of 
// pointer and ptrdiff_t, is returned in rax as it should be

// Whatever the reason for this, we need to follow the different struct
// calling conventions on Windows

// TODO: 
// Keep rState in rArg1. Because all functions we call take State& as the first argument, this will
// remove the need for some MOV instructions. 

// TODO: GC frames can be trivially omitted in certain cases; if a function
// does not have upvalues, never calls other functions or allocation primitives

// A function like say, memv or assq might benefit heavily from this, but we'd probably need to
// inline recursive calls in the Scheme compiler in order to make it easy to determine whether this
// is possible here.

// TODO: Currently variables are GC-tracked much in the way we track them C++-side. But in assembly
// we have full control over the stack. Rather than using the NativeFrame structure, for example,
// we could just guarantee that everything from rsp to rsp+whatever is a pointer. One issue with the
// current approach vs VMFrames is that all stack values will be tracked while a function is live, 
// which means some garbage will remain. Imagine something like this being a memory leak:
// (let ()
//   ;; really complex computation that makes a big list
//   ;; main loop of a program that doesn't clear the stack
// )
// In practice, probably not a big deal. But worth changing if possible

// Could also just zero the stack when convenient

// TODO: Exception checking

// TODO: Function application
// TODO: Type checking
// TODO: Conditionals

// TODO: Serializability. Currently we're encoding function pointers directly in compiled code.

// TODO: We also may need to deal with C++ exceptions. Although we don't use them, we want to be
// able to call out to C++ code that may

// TODO: How to get stack traces out of this?
// Well, we know where the code_offset is at any given time, so we could store that in a register
// or on the stack and use it along with the VM source information to add stack trace info.

// Also, we only really need to store it at spots that can cause errors. Something like
// PUSH_IMMEDIATE will never fail.

// TODO: Register allocation

// Anatomy of a natively-compiled Scheme function

// Registers:
// rState = r12 = pointer to arete::State*
// rStackI = r13 = during execution, stores stack index.
// after control ends (due to exception or return), stores return value

// Stack:
// 0-8 = pointer to previous native stack frame (if any)
// 8-16 = count of gc'd values
// 16-a = computation stack
// a-b = local values
// b-c = upvalues

// Stack
// rsp+0 = pointer to previous stack frame
// rsp+8 = size_t count of gc'd values
// rsp+16 = vmfunction pointer
// rsp+24 = closure pointer (if any)
// rsp+32 = gc scratch for debugging purposes
// 32 - kUpvalueOffset = local variables
// kUpvalueOffset - kCompStackOffset = upvalues
// kCompStackOffset - rsp = computation stack

// TODO: Use offsetof and other stuff instead of calling functions to retrieve information
// about a particular State


#include "dasm_proto.h"
#include "dasm_x86.h"

#define AR_LOG_JIT(msg) ARETE_LOG((ARETE_LOG_TAG_JIT), "jit", msg)

namespace arete {

static const bool dbg = false;
static const bool sanity_log = true;
static const bool gc_debug = true;

DefunGroup jit("jit");

///// A bunch of silly debugging code
Value state_make_pair(State* state) {
  Value pare(state->make_pair(C_FALSE, C_FALSE));
  std::cout << "state_make_pair returning " << (size_t) pare.bits << std::endl;
  return pare;
}

void state_collect(State* state) {
  std::cout << "COLLECTION BEGINNING " << (ptrdiff_t) state << std::endl;

  state->gc.collect();

  std::cout << "COLLECTION RETURNING" << std::endl;
}

static ptrdiff_t rsp = 0;

void sanity_log_1(ptrdiff_t rsp_) {
  rsp = rsp_;
  std::cout << "sanity_log_1: rsp after stack allocation: " << rsp << std::endl;
}

void sanity_log_2(ptrdiff_t compstack) {
  std::cout << "sanity_log_2: location of computation stack[0]: " << compstack << "(" << (rsp - compstack) << " from rsp)" << std::endl;
}

void sanity_log_3(ptrdiff_t locals) {
  std::cout << "sanity_log_3: location of locals[0]: " << locals << "(" << (rsp - locals) << " from rsp)" << std::endl;
}

void sanity_log_4(ptrdiff_t vfn) {
  std::cout << "sanity_log_4 vfn: " << vfn << std::endl;
}

void sanity_log_5(ptrdiff_t argc) {
  std::cout << "sanity_log_5: argc: " << argc << std::endl;
}

void sanity_log_6(ptrdiff_t argv) {
  std::cout << "sanity_log_6: argv ptr: " << argv << std::endl;
}

static size_t vfn_bits = 0;

// Check that parameters have been properly extracted
void sanity_log_7(ptrdiff_t s, ptrdiff_t ac, ptrdiff_t av, void** rsp) {
  std::cout << "sanity_log_7 state: " << s << " argc: " << ac << " argv: " << av << " fn: " << (ptrdiff_t)rsp << std::endl;
}

void sanity_log_fourth_argument(void** rsp) {
	std::cout << "FOURTH:" << (size_t) rsp << std::endl;
	/*
	for(size_t i = 0; i != 12; i++) {
		std::cout << "[rsp+" << (i*8) << "] => " << (ptrdiff_t) rsp[i] << std::endl;
	}
	*/
}

void sanity_log_ret(ptrdiff_t rStackI) {
  std::cout << "sanity_log_ret: rStackI = " << rStackI << std::endl;
}

void sanity_log_end(ptrdiff_t rStackI) {
  std::cout << "sanity_log_end: return value = " << rStackI << std::endl;
}

void print_stack_frame(NativeFrame* frame) {
  std::cout << "<<" << std::endl;;
  std::cout << "stack frame " << (ptrdiff_t) frame << std::endl;
  std::cout << "previous " << (ptrdiff_t) frame->previous << std::endl;
  std::cout << "count " << (ptrdiff_t) frame->value_count << std::endl;
  std::cout << "vmfunction " << (ptrdiff_t) frame->values[0].bits << std::endl;
  std::cout << "closure " << (ptrdiff_t) frame->values[1].bits << std::endl;
  std::cout << "scratch " << (ptrdiff_t) frame->values[2].bits << std::endl;
  for(size_t i = 3; i < frame->value_count; i++) {
    std::cout << "values[" << i << "]: " << (ptrdiff_t) frame->values[i].bits << std::endl;
  }
  std::cout << ">>" << std::endl;;
}

///// BELOW HERE: Functions which are actually needed for code to run properly

// Exception handling

// Because string munging in assembly is painful, exceptions are described by the NativeException
// struct and generated through an external C++ procedure

// Exception tag codes
enum {
  E_TYPE,
  E_EVAL
};

// Exception source codes
enum {
  E_UPVALUES_NOT_SUPPORTED,
  E_UPVALUE_CLOSED,
  E_UPVALUE_NATIVE,
  E_UPVALUE_VM,
  E_UNDEFINED,
  E_ARITY_CHECK_EQ,
  E_ARITY_CHECK_GTE,
  E_APPLY_CHECK,
  E_ADD,
  E_SUB,
  E_FX_ADD,
  E_FX_SUB,
  E_FX_LT,
};

// Exception message codes
enum {
  E_EXPECTED_FIXNUM_1,
  E_EXPECTED_FIXNUM_2,
};

struct NativeException {
  State* state1;
  State* state;
  ptrdiff_t function1;
  size_t code_offset;
  ptrdiff_t tag;
  ptrdiff_t src;
  ptrdiff_t arg1;
  ptrdiff_t arg2;
};

/**
 * Function for generating an exception with a trace from a native function.
 */
ptrdiff_t native_throw_exception(NativeException* exc) {
	std::cout << sizeof(NativeException) << std::endl;
  std::cout << (ptrdiff_t) exc << std::endl;
  if(!exc) return C_FALSE;

  AR_LOG_JIT("!native_throw_exception called with state " << (ptrdiff_t) exc->state <<
    " VMFunction: " << exc->function1 << " code_offset: " << exc->code_offset
    << " tag: " << exc->tag << " src: " << exc->src << " arg1: " << exc->arg1
    << " arg2: " << exc->arg2);

	Value function(exc->function1);

  AR_ASSERT(function.type() == VMFUNCTION);

  std::ostringstream os;

  Value stag;

  if(exc->tag == E_TYPE) {
    stag = exc->state->globals[State::S_TYPE_ERROR];
  } else {
    stag = exc->state->globals[State::S_EVAL_ERROR];
  }

  bool check_code = true;
  switch(exc->src) {
    case E_UPVALUES_NOT_SUPPORTED: {
      check_code = false;
      os << "closures not supported :(";
      break;
    }

    case E_APPLY_CHECK: {
      check_code = false;
      os << "attempt to apply non-applicable value " << Value(exc->arg1);
      break;
    }

    case E_UNDEFINED: {
      check_code = false;
      os << "attempt to access undefined variable " << Value(exc->arg1);
      break;
    }

    case E_UPVALUE_NATIVE: check_code = false; os << "native upvalue"; break;
    case E_UPVALUE_CLOSED: check_code = false; os << "closed upvalue"; break;
    case E_UPVALUE_VM: check_code = false; os << "vm upvalue"; break;

    case E_ARITY_CHECK_EQ: {
      check_code = false;
      os << "expected exactly " << exc->arg1 << " arguments but got " << exc->arg2;
      break;
    }
    case E_ARITY_CHECK_GTE: {
      check_code = false;
      os << "expected at least " << exc->arg1 << " arguments but got " << exc->arg2;
      break;
    }
    case E_FX_ADD: os << "primitive fx+ "; break;
    case E_FX_SUB: os << "primitive fx- "; break;
    case E_ADD: os << "primitive + "; break;
    case E_SUB: os << "primitive - "; break;
    case E_FX_LT: os << "primitive fx< "; break;
    default: break;
  }

  if(check_code) {
    switch(exc->arg1) {
      case E_EXPECTED_FIXNUM_1: os << "expected argument 1 to be a fixnum"; break;
      case E_EXPECTED_FIXNUM_2: os << "expected argument 2 to be a fixnum"; break;
      default: break;
    }
  }

  std::cout << os.str() << std::endl;

  exc->state->trace_function(function, 0, exc->code_offset);
  
  return (ptrdiff_t)exc->state->make_exception(stag, os.str()).bits;
}

// callee-save

// parameters = rdi, rsi, rdx, rcx, r8, r9, xmm0-7
// rsp = stack pointer
// saved = rbx, rbp, rdi, rsi, rsp, r12-15
// rsp-128 is the red zone; can be used for temp values but destroyed by any called function

Value generic_apply(State* state, size_t argc, Value* argv, Value fn) {
  std::cout << "Generic_apply: " << (ptrdiff_t)state << ' ' << (ptrdiff_t) fn.bits <<
    ' ' << (ptrdiff_t) argc << ' ' << (ptrdiff_t) argv << std::endl;


  if(argc > 0) {
    std::cout << "argv[0] " << (ptrdiff_t) argv[0].bits << std::endl;
  }

  Value result(state->apply(fn, argc, argv));

  std::cout << state->get_symbol("print").symbol_value().bits << std::endl;

  return result;
}

void* allocate_rest_arguments(State* state, size_t arity, size_t argc, Value* argv) {
  std::cout << "allocating rest arguments list " << (ptrdiff_t) state << " arity: " << arity << ' ' << "rest_argc_begin: " << (ptrdiff_t) argc <<
    ' ' << (ptrdiff_t) argv << std::endl;

  AR_ASSERT(arity <= argc);

  state->temps.clear();
  for(size_t i = arity; i != argc; i++ ) {
    std::cout << argv[i] << std::endl;
    state->temps.push_back(argv[i]);
  }

  return (void*) state->temps_to_list(0).bits;
}

struct NativeFrameSize {
  NativeFrame* previous;
  size_t value_count;
  Value closure;
  Value vmfunction;
  Value scratch;
};

void upvalue_close(State* state, Value upval) {
  std::cout << "closing upvalue " << upval.bits <<  ' ' << upval.type() << std::endl;
  AR_ASSERT(state->gc.live(upval.heap));
  AR_ASSERT(upval.type() == UPVALUE);
  upval.upvalue_close();
}

size_t allocate_upvalue(State* state, Value* local) {
  Value upvalue = state->gc.allocate(UPVALUE, sizeof(Upvalue));
  upvalue.heap->set_header_bit(Value::UPVALUE_POINTER_BIT);
  upvalue.as_unsafe<Upvalue>()->U.local = local;
  AR_LOG_JIT("allocating upvalue " << upvalue.bits << " to stack location " << (size_t) local);

  return (size_t) upvalue.bits;

}

size_t make_closure(State* state, size_t upvalues, Value* stack_start) {
  Value storage, closure, vmf(stack_start[0]);
  AR_FRAME(*state, vmf, storage, closure, vmf);
  AR_LOG_JIT("make-closure upvalues: " << upvalues << " function: " << vmf);


  storage = state->make_vector_storage(upvalues);
  for(size_t i = 0; i != upvalues; i++) {
    state->vector_storage_append(storage, stack_start[1+i]);
    AR_ASSERT(stack_start[i+1].heap_type_equals(UPVALUE));
  }

  closure = state->gc.allocate(CLOSURE, sizeof(Closure));
  closure.as_unsafe<Closure>()->upvalues = storage.as<VectorStorage>();

  closure.as_unsafe<Closure>()->function = vmf;
  closure.procedure_install(vmf.as_unsafe<Procedure>()->procedure_addr);
  AR_ASSERT(closure.type() == CLOSURE);

  return closure.bits;
}

Value vmfunction_to_native(State& state, size_t argc, Value* argv, void* _) {
  const char* fn_name = "vmfunction->native";

  AR_FN_ARGC_GTE(state, argc, 1);
  AR_FN_ASSERT_ARG(state, 0, "to be a closure or vm function", argv[0].heap_type_equals(CLOSURE) || argv[0].heap_type_equals(VMFUNCTION));
  Value nfn, vfn, closure;

  closure = argv[0];
  vfn = closure.closure_unbox();

  AR_FRAME(state, nfn, vfn, closure);

  AR_LOG_JIT("vmfunction_to_native called");

  VMFunction* vmf = vfn.as_unsafe<VMFunction>();

  unsigned local_count = vmf->local_count;
  unsigned stack_max = vmf->stack_max;
  unsigned upvalue_count = vmf->upvalue_count;
  unsigned label = 0;

  // Using the otherwise not-used-in-this codebase kBlah style to denote things that will become
  // constants in the compiled code

  unsigned total_stack = local_count + stack_max + upvalue_count;
  unsigned kFrameValues = total_stack + 3;
  // Locals + computation stack + pointer to VMFunction and Closure and scratch

  size_t kFrameSize = sizeof(NativeFrameSize);
  // size_t kValueSize = sizeof(void*);

  // Stack frame offsets
  unsigned kLocalsOffset = kFrameSize;
  unsigned kUpvaluesOffset = kFrameSize + (local_count * sizeof(void*));
  unsigned kCompStackOffset = kFrameSize + ((local_count + upvalue_count) * sizeof(void*));

  unsigned kTotalStackSize = kFrameSize + (total_stack * 8);
  unsigned kTotalStackSizeAligned = kTotalStackSize;

  if(!((kTotalStackSizeAligned % 16) != 0)) {
    kTotalStackSizeAligned += 8;
  }

#ifdef _MSC_VER
  kTotalStackSizeAligned += 8;
#endif

  AR_LOG_JIT("function frame info. stack_max " << stack_max << " kTotalStackSize " << kTotalStackSize << \
    " kTotalStackSizeAligned " << kTotalStackSizeAligned << " kLocalsOffset " << kLocalsOffset << 
    " kUpvaluesOffset " << kUpvaluesOffset << \
    " kCompStackOffset " << kCompStackOffset \
		<< " kFrameValues " << kFrameValues << " kFrameSize " << kFrameSize);

  // A note on a possible source of confusion: in the virtual machine, the stack is a separately
  // allocated data structure from locals and upvalues. Thus there's the computation stack which
  // exists only metaphorically here and is where the values of temporary computation are stored and
  // tracked by the garbage collector, and the actual stack i.e. rsp.

  | .arch x64
  |.section code

  |.define rState, r12
  |.define rStackI, r13
	|.if WINDOWS
	| .define rArg1, rcx
	| .define rArg2, rdx
  | .define rArg3, r8
  | .define rArg4, r9
	|.else
  | .define rArg1, rdi
  | .define rArg2, rsi
  | .define rArg3, rdx
  | .define rArg4, rcx
	|.endif
  |.type state, State, rState


  // Defined separately in case calling conventions changed

  |.define sFramePrev, [rsp]
  |.define sFrameCount, [rsp+8]
  |.define sFrameFunction, [rsp+16]
  |.define sFrameClosure, [rsp+24]
  |.define sFrameScratch, [rsp+32]

  // Call a simple FUN, for debugging only.
  |.macro simpCall, fun
  | push rArg1; push rArg2; push rArg3; push rArg4;
	| .if WINDOWS
	| push rbp; mov rbp, rsp; mov64 rax, (ptrdiff_t) fun; call rax; mov rsp, rbp; pop rbp
	| .else
  | push rbp; push rsp; mov64 rax, (ptrdiff_t) fun; call rax; pop rsp; pop rbp;
	| .endif
  | pop rArg4; pop rArg3; pop rArg2; pop rArg1
  |.endmacro
  
  // Call without saving any registers
  | .macro callNoSave, fun
  | push rbp; push rsp; mov64 rax, (ptrdiff_t) fun; call rax; pop rsp; pop rbp
  | .endmacro

  // Call a function which takes arete::State as its first argument
  // and returns a Value
	|.macro stateCallRetValue, method
	// On Windows, a Value is returned as a pointer to a stack value. Can't find
	// documentation for this anywhere, but both Clang and MSVC do it; which
	// means a little dance is needed to get it into rax.
  | .if WINDOWS
  |  push rbp
  |  mov rbp, rsp
  |  mov rArg1, rState
  |  mov64 rax, (ptrdiff_t) method
	|  sub rsp, 32
  |  call rax
	|  add rsp, 32
	|  mov rArg1, [rax]
	|  mov rax, rArg1
	|  mov rsp, rbp
  |  pop rbp
  | .else
  |  push rbp
  |  push rsp
  |  mov rArg1, rState
  |  mov64 rax, (ptrdiff_t) method
  |  call rax
  |  pop rsp
  |  pop rbp
  | .endif
	|.endmacro

  // Call a function which takes arete::State as its first argument
  |.macro stateCall, method
  | .if WINDOWS
  |  push rbp
  |  mov rbp, rsp
  |  mov rArg1, rState
  |  mov64 rax, (ptrdiff_t) method
	|  sub rsp, 32
  |  call rax
	|  add rsp, 32
	|  mov rsp, rbp
  |  pop rbp
  | .else
  |  push rbp
  |  push rsp
  |  mov rArg1, rState
  |  mov64 rax, (ptrdiff_t) method
  |  call rax
  |  pop rsp
  |  pop rbp
  | .endif
  |.endmacro

  // Check that value in STORE is a valid fixnum, if not, generate a type error with SRC and CODE
  // Generates a label
  |.macro checkFixnumP, store, src, code
  | test qword store, 1
  | jnz >1
  // Test failed. Set up registers for exception
  | throwNativeException (code_offset-1), E_TYPE, src, code, 0
  |1:
  |.endmacro

  |.macro checkFixnumPReg, store, src, code
  | test store, 1
  | jnz >1
  // Test failed. Set up registers for exception
  | throwNativeException (code_offset-1), E_TYPE, src, code, 0
  |1:
  |.endmacro
  |.macro checkNumberP, store, src, code
  |.endmacro

  // jmp to label >1 if value in STORE is immediate
  |.macro immediateJmpTo1, store
  // bits == 0 is #f
  | test store, store
  | je >1
  // bits & 3
  | test store, 3
  | jnz >1
  |.endmacro

  // Check whether a value in STORE is an exception and return it if so
  |.macro checkExceptionP, store
  | immediateJmpTo1 store
  | mov rax, [store]
  // quick exception check: we check for exception TYPE and 1 << 10 EXCEPTION_ACTIVE_BIT at once.
  | and rax, 2047
  | cmp rax, (EXCEPTION + Value::EXCEPTION_ACTIVE_BIT)
  | jne >1
  // this is an exception, return immediately
  | mov rArg3, code[current_insn]
  | mov rStackI, store
  | jmp ->trace_exception
  |1:
  |.endmacro

  // Check whether a value in STORE is applicable and extract its procedure address if so
  |.macro extractProcedureAddr, store
  // This is now guaranteed to be a heapvalue pointer, test it for VALUE_PROCEDURE_BIT
  | immediateJmpTo1 store
  | test qword [store], Value::VALUE_PROCEDURE_BIT
  | jnz >2
  | 1:
  | throwNativeException current_insn, E_EVAL, E_APPLY_CHECK, store, 0
  | 2:
  | mov rax, store
  | add rax, (&((Procedure*)0)->procedure_addr)
  | mov rax, [rax]
  |.endmacro

  |.macro popStackTop, store
  | sub rStackI, 8
  | mov store, rStackI
  | mov store, [store]
  |.endmacro

  |.macro getStackTop, store
  | mov store, rStackI
  | sub store, 8
  | mov store, [store]
  |.endmacro

  |.macro throwNativeException, offset, tag, src, arg1, arg2
  // See also ->exception2:
  | sub rsp, sizeof(NativeException)
  | mov qword [rsp+24], offset
  | mov qword [rsp+32], tag
  | mov qword [rsp+40], src
  | mov qword [rsp+48], arg1
  | mov qword [rsp+56], arg2
  | jmp ->exception
  |.endmacro

  // store a constant in a register
  |.macro storeConstant, reg, idx
  | mov rArg2, [rsp+16]
  // Get &VMFunction::constants
  | add rArg2, (ptrdiff_t)(&((VMFunction*)0)->constants)
  // Get &VMFunction->constants->data[idx]
  | mov rArg2, [rArg2]
  // Get VMFunction->constants->data[idx]
  | add rArg2, (ptrdiff_t)((((ptrdiff_t)&((VectorStorage*)0)->data)) + (idx * 8))
  | mov reg, [rArg2]
  |.endmacro

  |.macro incStack
  | add rStackI, 8
  |.endmacro

  |.macro decStack
  | sub rStackI, 8
  |.endmacro

  |.macro pushConstant, idx
  | storeConstant rArg2, idx
  | mov [rStackI], rArg2
  | incStack
  |.endmacro

  dasm_State *d;
  dasm_init(&d, DASM_MAXSECTION);

  |.actionlist actions
  |.globals globals
  |.globalnames globalnames

  void* globals[globals_MAX == 0 ? 1 : globals_MAX];
  (void) globalnames; // suppress compiler warning

  dasm_setupglobal(&d, globals, globals_MAX);
  dasm_setup(&d, actions);

  dasm_State** Dst = &d;

  |.code
		//
  // Prologue
  // Save registers
	| push rbp
  | mov rbp, rsp
  | push rbx
  | push rState
  | push rStackI
	|.if WINDOWS
	| push r14
	| push r15
	| mov rState, rArg2
	|.else
  | mov rState, rArg1
	|.endif

	// Get fourth argument off weird place in stack, thanks Windows.
	// 56 + (8*5) = register amount
	|.if WINDOWS
	// r14 = pointer to Closure/VMFunction
	| mov r14, [rsp+56+(8*6)]
	// r15 = location of return value on stack
	| mov r15, rArg1
	|.endif

  // NOTE: At the boundary of any CALL, stack size must be a multiple of 16!
  // PUSHing grows it by only 8.

  // Allocate stack space
  | sub rsp, kTotalStackSizeAligned

  // Get stack pointer
  | mov rStackI, rsp
  | add rStackI, kCompStackOffset

  // NativeFrame.values[0] = vfn
	|.if WINDOWS
	| mov qword sFrameFunction, r14
	|.else
  | mov sFrameFunction, rArg4
	|.endif
  // NativeFrame.values[1] = closure
  | mov qword sFrameClosure, 18
  | mov qword sFrameScratch, 18

	|.if WINDOWS
	// Windows, args have already been shifted because of screwed up calling convention
	| mov rArg3, rArg4
	| mov rArg4, r14
	|.else
  // Shuffle argc, argv into different registers so they can be processed in the function body
  // Shuffle argv into different register so we can use rArg2
  | mov rArg4, rArg3
  | mov rArg3, rArg2
	|.endif

  // Extract closure, if there is one

  // TODO: We know at compile time whether or not a VMFunction is a closure. Need to add a flag
  // or something so we don't emit this on non-closures
  | mov rax, sFrameFunction
  | mov rax, [rax]
  | and rax, 255
  | cmp rax, (CLOSURE)
  | jne ->past_closure_check

  | mov rax, sFrameFunction
  | mov sFrameClosure, rax

  | add rax, (&((Closure*)0)->function)
  | mov rax, [rax]
  | mov sFrameFunction, rax

  | ->past_closure_check:

  // NativeFrame.previous = state->gc.native_frames
  | mov rArg1, rState
  | add rArg1, (ptrdiff_t)(&((arete::State*)0)->gc.native_frames)
  | mov rArg2, [rArg1]
  | mov sFramePrev, rArg2
  | mov [rArg1], rsp

  // NativeFrame.value_count
  | mov qword sFrameCount, kFrameValues

  if(dbg && sanity_log) {
    | mov rArg1, rsp
    | simpCall sanity_log_1

    | mov rArg1, rStackI
    | simpCall sanity_log_2

    | mov rArg1, rsp
    | add rArg1, kLocalsOffset
    | simpCall sanity_log_3

    | mov rArg1, sFrameFunction
    | simpCall sanity_log_4

    | mov rArg1, rArg3
    | simpCall sanity_log_5

    | mov rArg1, rArg4
    | simpCall sanity_log_6

		| push rArg1; push rArg2; push rArg3; push rArg4;
		| mov rArg1, rState
		| simpCall sanity_log_7
		| pop rArg4; pop rArg3; pop rArg2; pop rArg2;
  }

  // Zero out GC frame
  // Assumes function has a stack size of at least one.

  // TODO: Not necessary to zero out whole frame

  // Should be able to just zero out upvalues + computation stack. 
  // Locals (except rest) will be initialized before any collection

  // TODO: Except what if a function call causes an exception e.g. an arity error?
  // In that case, we should pop the native_frame off?

  | mov rArg1, rsp
  | add rArg1, kFrameSize
  // | add rArg1, kLocalsOffset + (vmf->max_arity * 8)

  | mov rArg2, rsp
  | add rArg2, kTotalStackSizeAligned

  | ->gc_zero:
  | mov qword [rArg1], 0
  | add rArg1, 8
  | cmp rArg1, rArg2
  | jne ->gc_zero


  if(upvalue_count) {
    for(size_t i = 0; i != upvalue_count; i++) {
      size_t idx = vmf->free_variables->bv_ref<size_t>(i);

      AR_LOG_JIT("allocating upvalue[" << i << "] from local idx " << idx);

      | mov rArg1, rState
      | mov rArg2, rsp
      | add rArg2, (kLocalsOffset + (idx * 8))

      | push rArg3; push rArg4;
      | push rbp; push rsp;
      | mov64 rax, (size_t) allocate_upvalue; call rax
      | pop rsp; pop rbp;
      | pop rArg4; pop rArg3

      | mov rArg1, rsp
      | add rArg1, (kUpvaluesOffset + (i * 8))
      | mov [rArg1], rax
    }

    if(dbg) {
      | mov rArg1, rsp
      | simpCall print_stack_frame
    }

    //| throwNativeException 0, E_EVAL, E_UPVALUES_NOT_SUPPORTED, 0, 0
  }

  if(dbg) {
    | mov rArg1, rsp
    | simpCall print_stack_frame
  }

  // Execute function body
  size_t code_offset = 0;
  size_t current_insn = 0;
  size_t *code = vmf->code_pointer();
  bool done = false;
  size_t code_limit = vmf->code->length;

  std::unordered_map<size_t, unsigned> offsets_to_labels;

  // Generate all labels

  // Generate labels
  while(true) {
    size_t loc = 0;

    if(code_offset == code_limit) {
      break;
    }

    switch(code[code_offset++]) {
      case OP_NOT:
      case OP_FX_ADD: case OP_FX_SUB: 
      case OP_FX_LT:
      case OP_POP:
      case OP_RETURN: 
      case OP_ARGV_REST: continue;

      case OP_APPLY:
      case OP_APPLY_TAIL:
      case OP_PUSH_IMMEDIATE:
      case OP_GLOBAL_GET:
      case OP_UPVALUE_GET:
      case OP_UPVALUE_SET:
      case OP_LOCAL_GET:
      case OP_LOCAL_SET:
      case OP_UPVALUE_FROM_LOCAL:
      case OP_UPVALUE_FROM_CLOSURE:
      case OP_CLOSE_OVER:
      case OP_ARGC_EQ:
      case OP_ARGC_GTE:
      case OP_PUSH_CONSTANT:
      case OP_ADD: case OP_SUB:
        code_offset++; continue;

      case OP_GLOBAL_SET: code_offset += 2; continue;

      case OP_JUMP:
      case OP_JUMP_WHEN:
      case OP_JUMP_UNLESS:
      case OP_JUMP_WHEN_POP: {
        loc = code[code_offset++];
        break;
      }
      default: {
        AR_LOG_JIT("label generation unknown bytecode " << code[code_offset-1]);
        return state.eval_error("unknown bytecode during label generation");
      }
    }

    if(loc > 0){
      auto it = offsets_to_labels.find(loc);
      if(it == offsets_to_labels.end()) {
        unsigned lbl = label++;
        AR_LOG_JIT("code offset " << loc << " => label " << lbl);
        offsets_to_labels.insert(std::make_pair(loc, lbl));
      } else {
        AR_LOG_JIT("code offset " << loc << " == label " << it->second);
      }
    }
  }

  std::vector<size_t> offsets;
  for(auto it : offsets_to_labels) {
    offsets.push_back(it.first);
  }

  std::sort(offsets.begin(), offsets.end(), std::greater<size_t>());

  // Allocate space for labels
  dasm_growpc(&d, label);

  done = false;
  code_offset = 0;

  while(!done) {
    if(code_offset == code_limit) {
      break;
    }
    // Test for label generation
    if(offsets.size() > 0 && code_offset == offsets[offsets.size() - 1]) {
      auto i = offsets_to_labels.find(code_offset);
      unsigned lbl = i->second;
      offsets.pop_back();
      AR_LOG_JIT("generating label " << lbl << " for code offset " << code_offset);
      |=>lbl:
    }

    // Keep track of the current VM instruction
    // This is used to generate exception traces
    current_insn = code_offset;

    switch(code[code_offset++]) {
      case OP_PUSH_IMMEDIATE: {
        size_t value = code[code_offset++];
        AR_LOG_JIT("push-immediate " << Value(value) << " (" << (ptrdiff_t) value << ")");
        | mov qword [rStackI], (size_t) value
        | add rStackI, 8
        break;
      }

      case OP_PUSH_CONSTANT: {
        size_t idx = code[code_offset++];
        AR_LOG_JIT("push-constant " << idx);

        | pushConstant idx
        break;
      }

      case OP_POP: {
        AR_LOG_JIT("pop");
        | sub rStackI, 8
        break;
      }

      case OP_GLOBAL_GET: {
        size_t idx = code[code_offset++];

        AR_LOG_JIT("global-get " << idx);

        | storeConstant rArg1, idx
        | add rArg1, (ptrdiff_t)(&(((Symbol*)0)->value))
        | mov rArg1, [rArg1]

        | cmp rArg1, (size_t)C_UNDEFINED
        | jne >1
        | storeConstant rArg1, idx
        | throwNativeException (size_t) current_insn, E_EVAL, E_UNDEFINED, rArg1, 0
        | 1:
        | mov [rStackI], rArg1
        | incStack

        break;
      }

      case OP_LOCAL_GET: {
        size_t idx = code[code_offset++];
        size_t stack_offset = (kLocalsOffset + (8 * idx));
        AR_LOG_JIT("local-get " << idx << " at stack location " << stack_offset);

        | mov rArg1, rsp
        | add rArg1, stack_offset

        | mov rArg1, [rArg1]

        | mov [rStackI], rArg1
   
        | incStack
        break;
      }

      case OP_LOCAL_SET: {
        size_t idx = code[code_offset++];
        size_t stack_offset = (kLocalsOffset + (8 * idx));
        AR_LOG_JIT("local-set " << idx << " at stack location " << stack_offset);

        | mov rArg1, rsp
        | add rArg1, stack_offset

        | popStackTop rArg2
        | mov [rArg1], rArg2

        break;
      }

      case OP_GLOBAL_SET: {
        size_t error_on_undefined = code[code_offset++];
        size_t idx = code[code_offset++];

        AR_LOG_JIT("global-set " << error_on_undefined << " " << idx);
        | storeConstant rArg1, idx
        | add rArg1, (ptrdiff_t)(&(((Symbol*)0)->value))

        | popStackTop rArg2
        | mov [rArg1], rArg2

        break;
      }

      case OP_UPVALUE_GET: {
        size_t idx = code[code_offset++];

        AR_LOG_JIT("upvalue-get " << idx);

        |.macro getUpvalue, store, idx
        | mov store, sFrameClosure
        | add store, (&((Closure*)0)->upvalues)
        | mov store, [rax]
        | add store, (ptrdiff_t)((((ptrdiff_t)&((VectorStorage*)0)->data)) + (idx * 8))
        | mov store, [rax]
        |.endmacro

        // Jumps to labels based on type of upvalue in rax
        // If upvalue is closed, jumps to >2
        // If upvalue is native, jumps to >1
        // If upvalue is VM, code immediately afterwards is executed
        |.macro upvalueDispatch
        | mov rArg1, [rax]
        | and rArg1, (Value::UPVALUE_CLOSED_BIT)
        | cmp rArg1, (Value::UPVALUE_CLOSED_BIT)
        | je >2
        | mov rArg1, [rax]
        | and rArg1, (Value::UPVALUE_POINTER_BIT)
        | cmp rArg1, (Value::UPVALUE_POINTER_BIT)
        | je >1
        |.endmacro

        |.macro upvalueExtract
        | mov rArg2, rax
        | add rArg2, (ptrdiff_t)(&((Upvalue*)0)->U.local)
        | mov rArg2, [rArg2]
        |.endmacro


        // TODO: This adds a LOT of code. 

        // Extract upvalue contents, which may either be a pointer, a stack offset, or a value
        // to be used immediately

        | getUpvalue rax, idx
        | upvalueExtract
        | upvalueDispatch

        // The upvalue is an offset into the VM stack
        // Get pointer to VM stack
        | mov rArg1, rState
        | add rArg1, (ptrdiff_t)(&((arete::State*)0)->gc.vm_stack)
        | mov rArg1, [rArg1]
        // Get offset of VM stack
        | imul rArg2, 8
        | add rArg1, rArg2
        | mov rArg1, [rArg1]
        | mov [rStackI], rArg1
        | jmp >3
        | 1:
        | mov rArg2, [rArg2]
        | 2: 
        | mov [rStackI], rArg2
        | 3: 

        | incStack

        break;
      }

      case OP_UPVALUE_SET: {
        size_t idx = code[code_offset++];
        AR_LOG_JIT("upvalue-set " << idx);

        | getUpvalue rax, idx
        | upvalueDispatch
        //
        | throwNativeException 0, E_EVAL, E_UPVALUE_VM, 0, 0
        | jmp >1
        | 2:
        | throwNativeException 0, E_EVAL, E_UPVALUE_CLOSED, 0, 0
        | 1:
        | throwNativeException 0, E_EVAL, E_UPVALUE_VM, 0, 0
        | 3:
        | incStack

        break;
      }

      case OP_UPVALUE_FROM_CLOSURE: {
        size_t idx = code[code_offset++];

        | throwNativeException 0, E_EVAL, E_UPVALUES_NOT_SUPPORTED, 0, 0
        break;
      }

      case OP_UPVALUE_FROM_LOCAL: {
        size_t idx = code[code_offset++];

        AR_LOG_JIT("upvalue-from-local " << idx);

        | mov qword [rStackI], 18
        | mov rArg1, rsp
        | add rArg1, (kUpvaluesOffset + (idx * 8))
        | mov rArg1, [rArg1]
        | mov [rStackI], rArg1
        | incStack

        break;
      }

      case OP_CLOSE_OVER: {
        size_t upvalues = code[code_offset++];

        | mov rArg1, rState
        | mov64 rArg2, (size_t) upvalues
        | mov rArg3, rStackI
        // Stack position = stack - 8 - upvalues - 8 (for VMFunction at the beginning)
        | sub rArg3, (((size_t)((upvalues * 8) + 8)))
        | mov64 rax, (size_t) make_closure
        | call rax

        | sub rStackI, (((size_t)((upvalues * 8) + 8)))
        | mov [rStackI], rax
        | incStack

        | mov rArg1, rsp
        | simpCall print_stack_frame

        break;
      }

      case OP_APPLY_TAIL: 
      case OP_APPLY: {
        size_t argc = code[code_offset++];

        AR_LOG_JIT("apply " << argc);


        // Pointer to function on stack
        | mov rArg4, rStackI
        | sub rArg4, ((argc * 8) + 8)

        // Pointer to argv, if there is one
        if(argc > 0) {
          | mov rArg3, rArg4
          | add rArg3, 8
        }

        | mov rArg2, (size_t)argc
        | mov rArg1, rState

        // update stack pointer to point to where function was
        | mov rStackI, rArg4

        // Get pointer object from stack
        | mov rArg4, [rArg4]

        // Type check and get Procedure::procedure_addr
        | extractProcedureAddr rArg4

        | call rax

        | mov rArg1, rax

        | checkExceptionP rArg1

        | mov [rStackI], rArg1
        | incStack

        break;
      }

      case OP_JUMP: {
        size_t loc = code[code_offset++];
        AR_LOG_JIT("jump " << loc);

        auto it = offsets_to_labels.find(loc);
        AR_ASSERT(it != offsets_to_labels.end() && "jump label was not generated correctly");
        unsigned lbl = it->second;

        | jmp =>lbl
        break;
      }

      case OP_JUMP_WHEN:
      case OP_JUMP_UNLESS:
      case OP_JUMP_WHEN_POP: {
        size_t loc = code[code_offset++];
        // Get top of stack
        // If it is false, 

        auto it = offsets_to_labels.find(loc);
        AR_ASSERT(it != offsets_to_labels.end() && "jump label was not generated correctly");
        unsigned lbl = it->second;
        AR_LOG_JIT("jump-" << ((code[current_insn] == OP_JUMP_UNLESS) ? "unless" : "when") << ' ' << loc << " (label " << lbl << ")");

        if(code[current_insn] == OP_JUMP_WHEN_POP) {
          | popStackTop rArg1
        } else {
          | getStackTop rArg1
        }

        | cmp rArg1, C_FALSE
        if(code[current_insn] == OP_JUMP_UNLESS) {
          | jne =>lbl
        } else {
          | je =>lbl
        }
        break;
      }

      case OP_RETURN: {
        AR_LOG_JIT("return");
        | sub rStackI, 8
        | mov rStackI, [rStackI]

        | jmp ->unwind

        break;
      }

      // Checking function arity

      case OP_ARGC_EQ:
      case OP_ARGC_GTE: {
        size_t fargc = code[code_offset++];

        | cmp rArg3, (size_t) fargc
        if(code[current_insn] == OP_ARGC_EQ) {
          | je >1
        } else if(code[current_insn] == OP_ARGC_GTE) {
          | jge >1
        }
        | throwNativeException current_insn, E_EVAL, (code[current_insn] == OP_ARGC_EQ ? E_ARITY_CHECK_EQ : E_ARITY_CHECK_GTE), fargc, rArg3
        | 1:

        AR_LOG_JIT((code[current_insn] == OP_ARGC_EQ ? "argc-eq" : "argc-gte") << ' ' << fargc);

        // backup argv in r10 in case of rest arguments

        // Initialize local variables
        if(fargc) {
          AR_LOG_JIT("initializing locals with loop to " << kLocalsOffset + (8 * fargc));
          // rArg1 = &locals[0]
          | mov rArg1, rsp
          | add rArg1, (kLocalsOffset)
          // rArg2 = &argv[0]
          | mov rArg2, rArg4

          // limit = &locals[max_arity]
          | mov rax, rArg1
          | add rax, (8 * fargc)

          |->local_init:
          // mov argv[i] into r10
          | mov r10, [rArg2]
          // mov r10 into locals[i]
           | mov [rArg1], r10
          // Increment and check again
          | add rArg2, 8
          | add rArg1, 8
          | cmp rArg1, rax
          | jne ->local_init
        }

        if(dbg) {
          | push rArg3; push rArg4;
          if(gc_debug) {
            | stateCall state_collect;
          }
          | mov rArg1, rsp
          | simpCall print_stack_frame
          | pop rArg4; pop rArg3
        }

        | mov rArg2, (size_t) fargc


        break;
      }

      case OP_ARGV_REST: {
        AR_LOG_JIT("argv-rest");
        | mov rArg1, rState
        | mov64 rax, (size_t)allocate_rest_arguments
        | call rax

        | mov rArg1, rsp
        | add rArg1, (kLocalsOffset + (vmf->max_arity * 8))
        | mov [rArg1], rax

        break;
      }

      case OP_NOT: {
        | decStack

        | mov rArg1, [rStackI]
        | cmp rArg1, C_FALSE
        | je >1
        | mov qword [rStackI], C_FALSE
        | jmp >2
        | 1:
        | mov qword [rStackI], C_TRUE
        | 2:

        | incStack
        break;
      }

      // Fixnum boolean expressions
      case OP_FX_LT: {
        | decStack

        | mov rArg2, rStackI

        | mov rArg1, rStackI
        | sub rArg1, 8

        | mov rArg1, [rArg1]
        | mov rArg2, [rArg2]

        | checkFixnumPReg rArg2, E_FX_LT, E_EXPECTED_FIXNUM_2
        | checkFixnumPReg rArg1, E_FX_LT, E_EXPECTED_FIXNUM_1

        | decStack
        | mov qword [rStackI], C_FALSE
        | cmp rArg1, rArg2
        | jl >1
        | jmp >2
        | 1: 
        | mov qword [rStackI], C_TRUE
        | 2:
        | incStack

        break;
      }

      // Fixnum arithmetic operations
      case OP_FX_ADD:
      case OP_FX_SUB: {
        unsigned src = 0;

        switch(code[current_insn]) {
          case OP_FX_ADD: src = E_FX_ADD; AR_LOG_JIT("fx+"); break;
          case OP_FX_SUB: src = E_FX_SUB; AR_LOG_JIT("fx-"); break;
        }

        | sub rStackI, 8
        | mov rArg2, rStackI

        // TODO: Why is the shifting necessary? It should be possible to add or subtract
        // two fixnum values and return them as-is

        | mov rArg1, rArg2
        | mov rArg2, [rArg2]

        // 2 labels: two for each type test
        | checkFixnumPReg rArg2, src, E_EXPECTED_FIXNUM_2

        // Extract fixnum argument 1 value
        | shr rArg2, 1

        // Get first stackvalu
        | sub rArg1, 8

        | mov rax, [rArg1]
        | checkFixnumPReg rax, src, E_EXPECTED_FIXNUM_1
        | shr rax, 1

        switch(code[current_insn]) {
          case OP_FX_ADD:
            | add rax, rArg2
            break;
          case OP_FX_SUB:
            | sub rax, rArg2
            break;
        }

        | shl rax, 1
        | inc rax

        | mov [rArg1], rax

        // set top of stack
        break;
      }

      case OP_ADD:
      case OP_SUB: {
        size_t argc = code[code_offset++];
        AR_LOG_JIT("add " << argc);
        // for i = 0 to argc

        // while fixnum, accumulate in a register
        // if end of fixnump loop is reached, push value of register onto stack
        | mov rArg1, rStackI
        
        | sub rArg1, (((argc) * 8)) 
        | mov rArg2, [rArg1]
        | shr rArg2, 1
        | add rArg1, 8

        | ->add_fixnum:

        | mov rax, [rArg1]
        | and rax, 1
        | cmp rax, 1
        | jne ->add_flonum

        | mov rax, [rArg1]
        | shr rax, 1

        switch(code[current_insn]) {
          case OP_ADD: {
            | add rArg2, rax
            break;
          }
          case OP_SUB: {
            | sub rArg2, rax
            break;
          }
        }

        | add rArg1, 8
        | cmp rArg1, rStackI
        | jne ->add_fixnum

        | jmp ->good

        | ->add_flonum:
        | sub rStackI, 8
        | mov qword [rArg1], 0
        | jmp ->done

        | ->good:

        | shl rArg2, 1
        | inc rArg2

        | sub rStackI, ((argc) * 8)
        | mov [rStackI], rArg2
        | add rStackI, 8

        | ->done:

        // otherwise, check for fixnum or flonum, accumulate in flonum register
        // when end of loop is reached, push value of register onto stack
        break;
      }


      default: {
        AR_LOG_JIT("unknown bytecode " << code[code_offset-1]);
        return state.eval_error("unknown bytecode");
      }
    }
  }

  | jmp ->unwind

  // Attempt to trace exception
  |->trace_exception:

  |jmp ->unwind

  |->exception:
	// Add State* and VMFunction pointer to this
	| mov rArg1, [rsp+(sizeof(NativeException)+16)]
  | mov [rsp+16], rArg1
	| mov qword [rsp+8], rState
  | mov rArg1, rsp
	| sub rsp, 48
  | simpCall native_throw_exception
  | mov rStackI, rax
	| add rsp, 48
  | add rsp, sizeof(NativeException) // Reset stack
  
  |->unwind: 

  // Protect return value
  if(dbg && gc_debug) {
    | mov sFrameScratch, rStackI
    | stateCall state_collect;
    | mov rStackI, sFrameScratch
  }

  if(dbg && sanity_log) {
    | mov rArg1, rStackI
    | simpCall sanity_log_end
  }

  // Close over local upvalues
  if(upvalue_count) {
    for(size_t i = 0; i != upvalue_count; i++) {
      | mov rArg2, rsp
      | add rArg2, (kUpvaluesOffset + (i * 8))
      | mov rArg2, [rArg2]
      
      | mov rArg1, rState
      | mov64 rax, (size_t) upvalue_close
      | call rax
    }
  }

  // state->gc.native_frames = NativeFrame.previous
  | mov rArg1, rState
  | add rArg1, (ptrdiff_t)(&((arete::State*)0)->gc.native_frames)
  | mov rArg2, [rsp]
  | mov [rArg1], rArg2

  // Free stack space
  | add rsp, kTotalStackSizeAligned

  // Retrieve return value from rStackI
	| .if WINDOWS
	|  mov rax, rsp
	| .else
  |  mov rax, rStackI
	| .endif

  // Restore registers
	|.if WINDOWS
	| pop r15
	| pop r14
	|.endif
  | pop rStackI
	| pop rState
  | pop rbx
  | mov rsp, rbp
	| pop rbp

  // Create return value
  | ret

  size_t size;

  dasm_link(&d, &size);

  char* proc_addr = (char*) state.allocate_native_code(size);
  std::cout << (ptrdiff_t) proc_addr << std::endl;

  //bv = state.make_bytevector<unsigned char>(size);

  AR_LOG_JIT("function size: " << size << " vs " << (0 * 8));

  //dasm_encode(&d, bv.bv_data());
  dasm_encode(&d, proc_addr);

  dasm_free(&d);

  AR_LOG_JIT("vmfunction->native done; calling function");

  //vfn.as_unsafe<VMFunction>()->native_code = bv.as_unsafe<Bytevector>();
  vfn.heap->set_header_bit(Value::VMFUNCTION_NATIVE_BIT);

  // PROBLEM: A closure cannot have all procedure_addrs replaced in-place by the JIT compiler.

  vfn.as_unsafe<Procedure>()->procedure_addr = (c_closure_t)(void*)proc_addr;
  closure.as_unsafe<Procedure>()->procedure_addr = (c_closure_t)(void*)proc_addr;

  AR_ASSERT(vfn.procedurep());
  AR_ASSERT(closure.procedurep());

  return vfn;
}

AR_DEFUN("vmfunction->native!", vmfunction_to_native, 1);

Value fn_native_call(State& state, size_t argc, Value* argv, void* fn) {
  static const char* fn_name = "native-call";
  AR_FN_EXPECT_TYPE(state, argv, 0, VMFUNCTION); 

  AR_ASSERT(argv[0].heap->get_header_bit(Value::VMFUNCTION_NATIVE_BIT));

  Value vfn = argv[0];
  AR_FRAME(state, vfn);
  //AR_ASSERT(state.gc.live(vfn.as_unsafe<VMFunction>()->native_code));

  //Value bv(vfn.as_unsafe<VMFunction>()->native_code);

  if(dbg) {
    //std::cout << "Bv size: " << vfn.as_unsafe<VMFunction>()->native_code->size << std::endl;

    std::cout << "VMFunction ptr: " << (ptrdiff_t)vfn.bits << std::endl;
    std::cout << "Constants ref: " << (ptrdiff_t)&vfn.as<VMFunction>()->constants << std::endl;
    std::cout << "Constants ptr: " << (ptrdiff_t)vfn.as<VMFunction>()->constants << std::endl;
    std::cout << "Constants ptr[0]: " << (ptrdiff_t)&vfn.as<VMFunction>()->constants->data[0] << std::endl;

    std::cout << "#(";
    for(size_t i = 0; i != vfn.as<VMFunction>()->constants->length; i++) {
      std::cout << (ptrdiff_t)vfn.as<VMFunction>()->constants->data[i].bits;
      std::cout << ' ';
    }
    std::cout << ')' << std::endl;
    std::cout << "Calling against state: " << (ptrdiff_t) &state << std::endl;

    std::cout << " argc: " << argc-1 << " argv ptr: " << (size_t)argv+8 << std::endl;
  }

  // argc, argv, fn
	//void* fptr = (void*) vfn.as_unsafe<VMFunction>()->native_code->data;
  void *fptr = (void*) vfn.as_unsafe<VMFunction>()->procedure_addr;
  Value (*ptr)(State*, size_t, size_t, size_t) = (Value (*)(State*, size_t, size_t, size_t))(fptr);
  //Value result = ptr((State*) 12, (ptrdiff_t)34, (ptrdiff_t) 56, (ptrdiff_t) 78);
	vfn_bits = (size_t) vfn.bits;
	//for(size_t i= 0; i != 50; i++) { std::cout <<i<<std::endl;}
  Value result = ptr((State*) &state, argc-1, (size_t)argv+8, (size_t)vfn.bits);

  if(dbg) {
    std::cout << "result int: " << (ptrdiff_t)(result.bits) << std::endl;
    std::cout << "result: " << Value(result) << std::endl;

    std::cout << "state ptr: " << (ptrdiff_t)&state << std::endl;
    std::cout << "native_Frames: " << (ptrdiff_t)state.gc.native_frames << std::endl;
  }

  AR_ASSERT(state.gc.native_frames == 0);
  return result;
}
AR_DEFUN("native-call", fn_native_call, 1);

void load_native_compiler(State& state) {
  jit.install(state);
}

}

#endif // ARETE_64_BIT == 0




